{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "- All 11 qualitative questions analyzed for key topics\n",
    "- Number of topics varied within the recommended range of 2 - 5 based on unique key words for each group. This was a subjective process that had to be completed by adjusting the number of components, examing the key words, looking at the intertopic distance in the visualizations and reading the most relevant responses to each topic.\n",
    "- Non-cleaned text of must prominent topcs was included to make reading easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import nlp\n",
    "import wrangle\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "datadf, dictionarydf = wrangle.wrangle_data(path_prefix='../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datadf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datadf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of qualitative questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is your company or organization's primary industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and lemmatize the data for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.primary_industry = datadf.primary_industry.dropna().apply(nlp.basic_clean)\n",
    "datadf.primary_industry = datadf.primary_industry.dropna().apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word count matrix and vector. Settings: 2 word ngrams permitted; words in more than 30% of documents were ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_industry_matrix, primary_industry_vector = nlp.create_wordcount_matrix(datadf.primary_industry.dropna(), ngram=(1,3), max_df=.3)\n",
    "primary_industry_matrix, primary_industry_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply LDA method using 4, 6 and 8 components (can be changed) and a random state set to ensure the results can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda5 = LatentDirichletAllocation(n_components= 8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda5.fit(primary_industry_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda5, primary_industry_matrix, primary_industry_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. education \n",
    "2. fintech \n",
    "3. healthcare \n",
    "4. Tech \n",
    "\n",
    "#### Topic groups (6) \n",
    "1. fintech\n",
    "2. education\n",
    "3. healthcare\n",
    "4. technology\n",
    "5. software\n",
    "6. consultancy\n",
    "\n",
    "#### Topic groups (8) - 5 & 8 slight overlap\n",
    "1. fintech\n",
    "2. education\n",
    "3. technology\n",
    "4. consulting\n",
    "5. software\n",
    "6. healthcare\n",
    "7. government\n",
    "8. public (sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W5 = lda5.transform(primary_industry_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column5 = datadf.primary_industry.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create second word count matrix. Excludes words in appear in 80%+ of all documents and in 2 or fewer documents. Include bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix5, count_vect5 = nlp.create_wordcount_matrix(datadf.primary_industry, max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA5a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA5a.fit(word_count_matrix5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA5a.transform(word_count_matrix5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What types of research do you currently use to make decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.types_res_used = datadf.types_res_used.dropna().apply(nlp.basic_clean)\n",
    "datadf.types_res_used = datadf.types_res_used.dropna().apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_res_used_matrix, types_res_used_vector = nlp.create_wordcount_matrix(datadf.types_res_used.dropna(), ngram=(1,3), max_df=.3)\n",
    "types_res_used_matrix, types_res_used_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda6 = LatentDirichletAllocation(n_components= 8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda6.fit(types_res_used_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda6, types_res_used_matrix, types_res_used_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. card sort\n",
    "2. contextual inquiry\n",
    "3. focus group \n",
    "4. market rsearch\n",
    "\n",
    "#### Topic groups (6) \n",
    "1. diary study\n",
    "2. card (sort)\n",
    "3. contextual inquiry\n",
    "4. quantitative survey\n",
    "5. focus (group)\n",
    "6. generative (evaluative)\n",
    "\n",
    "#### Topic groups (8) - 8 overlaps with 2 & 6\n",
    "1. diary (study)\n",
    "2. contextual inquiry\n",
    "3. interview usability\n",
    "4. gernative evaluative\n",
    "5. market (research)\n",
    "6. ux research\n",
    "7. indepth (interview)\n",
    "8. concept validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W6 = lda6.transform(types_res_used_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column6 = datadf.types_res_used.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix6, count_vect6 = nlp.create_wordcount_matrix(datadf.types_res_used.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA6a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA6a.fit(word_count_matrix6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA6a.transform(word_count_matrix6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What types of research are you considering in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.future_res = datadf.future_res.dropna().apply(nlp.basic_clean)\n",
    "datadf.future_res = datadf.future_res.dropna().apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_res_matrix, future_res_vector = nlp.create_wordcount_matrix(datadf.future_res.dropna(), ngram=(1,3), max_df=.3)\n",
    "future_res_matrix, future_res_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda7 = LatentDirichletAllocation(n_components=7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda7.fit(future_res_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda7, future_res_matrix, future_res_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. ab testing\n",
    "2. quantitative\n",
    "3. diary study\n",
    "4. participatory\n",
    "\n",
    "#### Topic groups (6) - 4 & 6 overlap\n",
    "1. quantitative\n",
    "2. usability testing\n",
    "3. field\n",
    "4. analytics\n",
    "5. diary study\n",
    "6. journey\n",
    "\n",
    "#### Topic groups (8) - 5 is almost completely within 2; 6 & 8 overlap\n",
    "1. quantitative\n",
    "2. unmoderated usability\n",
    "3. diary study\n",
    "4. ab testing\n",
    "5. ux\n",
    "6. focus group\n",
    "7. ethnographic\n",
    "8. field study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W7 = lda7.transform(future_res_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column7 = datadf.future_res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix7, count_vect7 = nlp.create_wordcount_matrix(datadf.future_res.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA7a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA7a.fit(word_count_matrix7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA7a.transform(word_count_matrix7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column7, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Describe your educational background with research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.research_educ_desc = datadf.research_educ_desc.dropna().apply(nlp.basic_clean)\n",
    "datadf.research_educ_desc = datadf.research_educ_desc.dropna().apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_educ_desc_matrix, research_educ_desc_vector = nlp.create_wordcount_matrix(datadf.research_educ_desc.dropna(), ngram=(1,3), max_df=.3)\n",
    "research_educ_desc_matrix, research_educ_desc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda10 = LatentDirichletAllocation(n_components= 4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda10.fit(research_educ_desc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda10, research_educ_desc_matrix, research_educ_desc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4) - no overlapping circles\n",
    "1. participated\n",
    "2. grad school\n",
    "3. master degree\n",
    "4. pyschology\n",
    "\n",
    "#### Topic groups (6) - 1 &4, 2 & 3 overlap\n",
    "1. participated\n",
    "2. design research\n",
    "3. social science\n",
    "4. grad school\n",
    "5. human factor\n",
    "6. master degree\n",
    "\n",
    "#### Topic groups (8) - extreme overlap between topics 1, 2, 4 & 8, 54% of all the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W10 = lda10.transform(research_educ_desc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column10 = datadf.research_educ_desc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix10, count_vect10 = nlp.create_wordcount_matrix(datadf.research_educ_desc.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA10a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA10a.fit(word_count_matrix10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA10a.transform(word_count_matrix10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. How do you decide which events to attend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.how_pick_events = datadf.how_pick_events.dropna().apply(nlp.basic_clean)\n",
    "datadf.how_pick_events = datadf.how_pick_events.dropna().apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_pick_events_matrix, how_pick_events_vector = nlp.create_wordcount_matrix(datadf.how_pick_events.dropna(), ngram=(1,3), max_df=.3)\n",
    "how_pick_events_matrix, how_pick_events_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda14 = LatentDirichletAllocation(n_components= 6, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda14.fit(how_pick_events_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda14, how_pick_events_matrix, how_pick_events_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. pay\n",
    "2. topic\n",
    "3. location\n",
    "4. cost\n",
    "\n",
    "#### Topic groups (6) - overlap between 3 & 6\n",
    "1. pay\n",
    "2. value\n",
    "3. price\n",
    "4. reputation\n",
    "5. networking\n",
    "6. relevance\n",
    "\n",
    "#### Topic groups (8) - small amount of overlap among 2, 3, 4, & 6\n",
    "1. design\n",
    "2. affordable\n",
    "3. speaker topic\n",
    "4. reputation\n",
    "5. location cost\n",
    "6. value\n",
    "7. location price\n",
    "8. time away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W14 = lda14.transform(how_pick_events_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column14 = datadf.how_pick_events.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix14, count_vect14 = nlp.create_wordcount_matrix(datadf.how_pick_events.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA14a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA14a.fit(word_count_matrix14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA14a.transform(word_count_matrix14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column14, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. What was the best professional learning experience you've ever had?  What made it great?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf.best_event = datadf.best_event.dropna().apply(nlp.basic_clean)\n",
    "datadf.best_event = datadf.best_event.dropna().apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_event_matrix, best_event_vector = nlp.create_wordcount_matrix(datadf.best_event.dropna(), ngram=(1,3), max_df=.3)\n",
    "best_event_matrix, best_event_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda15 = LatentDirichletAllocation(n_components= 3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda15.fit(best_event_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda15, best_event_matrix, best_event_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4) - 3 is almost entirely enclosed by 2\n",
    "1. research\n",
    "2. networking\n",
    "3. day\n",
    "4. variety\n",
    "\n",
    "#### Topic groups (6) - 2 intersects with 1 & 4\n",
    "1. think\n",
    "2. practical\n",
    "3. learn\n",
    "4. design\n",
    "5. variety\n",
    "6. intimate\n",
    "\n",
    "#### Topic groups (8) - overlap between 1 & 2 and 3 & 4\n",
    "1. strive\n",
    "2. world\n",
    "3. concept\n",
    "4. uxpa\n",
    "5. immediately\n",
    "6. sxsw\n",
    "7. relevant\n",
    "8. start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Be Decided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W15 = lda15.transform(best_event_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column15 = datadf.best_event.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix15, count_vect15 = nlp.create_wordcount_matrix(datadf.best_event.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA15a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA15a.fit(word_count_matrix15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA15a.transform(word_count_matrix15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column15, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. What if any events have you attended on the subject of research in the past few years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['nan', 'Nan', 'NaN', 'NAN']\n",
    "\n",
    "stopWords = nlp.set_stop_words(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_wordcount_matrix(input_column, max_df=0.8, min_df=2, ngram=(1,1), stop_words='english'):\n",
    "    \"\"\"\n",
    "    Creates a feature matrix. Matrix is as wide as the terms that meet the min/max parameters. Each document/row\n",
    "    will have a wordcount for each term.\n",
    "    Can find ngrams, but has default set to 1-word ngrams. Set ngrams to (1,n) to look for ngrams.\n",
    "    \"\"\"\n",
    "    count_vect = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', ngram_range=ngram)\n",
    "    doc_term_matrix = count_vect.fit_transform(input_column.values.astype('U'))\n",
    "    return doc_term_matrix, count_vect\n",
    "\n",
    "word_count_matrix10, count_vect10 = create_wordcount_matrix(datadf.research_educ_desc.dropna(), max_df=0.8, min_df=2, ngram=(1,3), stop_words=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attend_recent = datadf.events_attend_recent.fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attend_recent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attend_recent.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attend_recent = events_attend_recent.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attend_recent = events_attend_recent.apply(nlp.basic_clean)\n",
    "events_attend_recent = events_attend_recent.apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_attend_recent_matrix, events_attend_recent_vector = create_wordcount_matrix(events_attend_recent, ngram=(1,3), max_df=.3, stop_words=stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_attend_recent_matrix, events_attend_recent_vector = nlp.create_wordcount_matrix(datadf.events_attend_recent.dropna(), ngram=(1,3), max_df=.3)\n",
    "# events_attend_recent_matrix, events_attend_recent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda16 = LatentDirichletAllocation(n_components= 7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda16.fit(events_attend_recent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda16, events_attend_recent_matrix, events_attend_recent_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic groups (4)\n",
    "\n",
    "1. day\n",
    "2. london\n",
    "3. epic\n",
    "4. uxpa\n",
    "\n",
    "#### Topic groups (6)\n",
    "1. london\n",
    "2. summit\n",
    "3. session\n",
    "4. design research\n",
    "5. epic\n",
    "6. uxr\n",
    "\n",
    "#### Topic groups (8)\n",
    "Heavy overlap among 6 of the 8 topics\n",
    "1. local meetups\n",
    "2. qrca\n",
    "3. design research\n",
    "4. uxpa\n",
    "5. attended\n",
    "6. epic\n",
    "7. user research\n",
    "8. focused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W16 = lda16.transform(events_attend_recent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column16 = datadf.events_attend_recent.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix16, count_vect16 = nlp.create_wordcount_matrix(datadf.events_attend_recent.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA16a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA16a.fit(word_count_matrix16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA16a.transform(word_count_matrix16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column16, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Did we miss any other types of conference sessions that you'd like to mention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_conference_types = datadf.other_conference_types.fillna('nan').apply(nlp.basic_clean)\n",
    "other_conference_types = other_conference_types.apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_conference_types.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_conference_types_matrix, other_conference_types_vector = nlp.create_wordcount_matrix(other_conference_types, ngram=(1,3), max_df=.3)\n",
    "other_conference_types_matrix, other_conference_types_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda20 = LatentDirichletAllocation(n_components= 3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda20.fit(other_conference_types_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda20, other_conference_types_matrix, other_conference_types_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. case study\n",
    "2. talk\n",
    "3. poster session\n",
    "4. nope\n",
    "\n",
    "#### Topic groups (6)\n",
    "1. reatreat\n",
    "2. working\n",
    "3. quality\n",
    "4. case study\n",
    "5. panel discussion\n",
    "6. tutorial\n",
    "\n",
    "#### Topic groups (8) - 3 is completely inside 1; 4 and 5 share about 75% of the same area\n",
    "1. working\n",
    "2. case study\n",
    "3. nice\n",
    "4. outside\n",
    "5. variety\n",
    "6. multitrack\n",
    "7. panel\n",
    "8. method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W20 = lda20.transform(other_conference_types_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column20 = datadf.other_conference_types.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix20, count_vect20 = nlp.create_wordcount_matrix(datadf.other_conference_types.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA20a = LatentDirichletAllocation(n_components=6, random_state=42)\n",
    "LDA20a.fit(word_count_matrix20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA20a.transform(word_count_matrix20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column20, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Subjects you most want to see covered at a research conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['nan', 'Nan', 'NaN', 'NAN']\n",
    "\n",
    "stopWords = nlp.set_stop_words(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_topics = datadf.ideal_topics.fillna('nan')\n",
    "ideal_topics = ideal_topics.apply(nlp.basic_clean)\n",
    "ideal_topics = ideal_topics.apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<726x1122 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 5854 stored elements in Compressed Sparse Row format>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=0.3, max_features=None, min_df=2,\n",
       "                 ngram_range=(1, 3), preprocessor=None,\n",
       "                 stop_words=['nan', 'Nan', 'NaN', 'NAN', 'am', 'however',\n",
       "                             'whatever', 'bottom', 'namely', 'most', 'whole',\n",
       "                             'since', 'among', 'than', 'anyone', 'hers', 'every',\n",
       "                             'yet', 'few', 'sixty', 'together', 'serious',\n",
       "                             'latter', 'find', 'cant', 'until', 'none', 'up',\n",
       "                             'he', 'meanwhile', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_topics_matrix, ideal_topics_vector = nlp.create_wordcount_matrix(ideal_topics, ngram=(1,3), max_df=.3, stop_words=stopWords)\n",
    "ideal_topics_matrix, ideal_topics_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda21 = LatentDirichletAllocation(n_components= 4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=4, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda21.fit(ideal_topics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el328611123078614489888181427\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el328611123078614489888181427_data = {\"mdsDat\": {\"x\": [-0.14885950140192172, 0.0762249196689366, 0.04480711802748427, 0.027827463705500852], \"y\": [-0.008165098688361936, 0.007506320854327366, -0.10635984136060095, 0.1070186191946355], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [28.152874889794738, 26.018037775811543, 23.396824323501516, 22.432263010892207]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [51.0, 58.0, 43.0, 127.0, 28.0, 46.0, 14.0, 20.0, 14.0, 25.0, 12.0, 23.0, 12.0, 16.0, 15.0, 15.0, 10.0, 10.0, 26.0, 22.0, 17.0, 61.0, 19.0, 9.0, 39.0, 11.0, 18.0, 38.0, 38.0, 16.0, 50.77767415371145, 43.06436483104446, 27.52421440782678, 54.774195661006154, 5.352453215401697, 5.3518734356842685, 4.4983322577832245, 4.4983322577824145, 4.495066188908329, 4.495043666397244, 4.495032039190078, 4.4816114086156285, 3.643183126975753, 3.6403008478142, 3.6398793407226426, 3.6398793407226337, 3.638438409230492, 2.786030071722642, 2.7860286149248794, 2.78601979300442, 2.7860067651002374, 2.7859938573022003, 2.7859917695088336, 2.7859744848849544, 2.7859561983296675, 2.7859432305206475, 2.784336352023377, 2.784336352023376, 2.782691400301595, 2.78199246834978, 13.720456670293872, 6.856300799871232, 12.635478278049835, 9.794628918469256, 4.51348271091647, 15.965184932671303, 22.18252048458478, 4.442885330946415, 11.464722721113732, 8.073447519355364, 12.425717317638977, 44.591409821177734, 12.354135784491675, 13.81925309583353, 12.78084786347377, 7.923564067143795, 6.21626886843444, 7.582157292767934, 7.497722817767982, 7.435000918370366, 10.322934353044197, 8.811051231791224, 7.848609722568069, 11.280205027700235, 11.647979260427359, 11.473370607065782, 10.141538415511908, 7.258348172115833, 7.850896578474987, 8.215789706278272, 7.688930893738589, 6.150860668567402, 4.455757650139397, 4.455339608067208, 4.452408400094531, 3.6067469401100465, 3.60663641224843, 3.5986663253417004, 3.575462092040129, 6.94189949023348, 2.7583142141188093, 2.7583052379576656, 2.758300845592541, 2.758296964473401, 2.7571673473050278, 2.757091157151241, 2.7565338514389803, 2.7562760650616216, 2.7541484873435933, 2.753006313092349, 2.7528371037488726, 2.7511671458465066, 2.7320202050823315, 12.006278181101997, 5.305930566272885, 5.207598215054015, 12.113708459363632, 1.909602551560161, 1.9096012636439772, 1.9095999177332352, 1.9095938085713757, 8.711788030232071, 11.13570757248953, 6.120732484747931, 31.38930658541574, 15.196005531997983, 12.25233499797126, 12.418670825311882, 9.366901479581804, 6.7444918253568185, 11.04042486679926, 6.343261385190638, 10.907399790102588, 6.161818630253312, 16.127799683411176, 16.356996898291666, 15.455935014255616, 4.440446128752902, 11.210694183525424, 13.432800166796373, 7.833792399623239, 13.267586660972677, 10.290581843236705, 14.296075279804347, 12.09675355368061, 18.044735883741687, 7.867540253656784, 7.835091665605343, 10.160932748075359, 9.085062668982555, 7.8436853594690925, 9.1810929259251, 7.900466437670185, 8.63206021478407, 6.075987071690217, 12.502423908261921, 4.408637362106846, 4.408637362106846, 16.61376341419273, 2.7291401899952743, 2.729098537879649, 2.728594999136498, 2.726312441574917, 2.7248207961148156, 2.723180313095555, 2.721642216767977, 2.7209376162795533, 2.7208728780514893, 2.717550144181771, 2.7080940161806755, 2.7051041478003093, 11.46481499312602, 4.411162268742794, 1.8894073440597174, 1.8894073440597174, 1.8894045230333638, 1.8894019270309321, 1.8894006097701068, 1.8894001416041906, 1.8894001416041906, 1.8893988172681186, 1.889398772062485, 1.8893970261858213, 1.8893946120671168, 6.9321133618775885, 4.4051504627776055, 4.930957256447047, 4.90883395274033, 3.5702644360241647, 3.569539511029782, 3.568743451200096, 15.403460306799552, 3.5543049043963997, 6.10213005704429, 6.092691464126366, 6.050828173521858, 8.367808186192423, 6.0509029600071935, 8.769816789409695, 5.928838869922902, 7.59049391491894, 6.850464192724074, 17.56330683550227, 14.477743375376871, 12.368128390282202, 19.710269044715986, 11.655410354793725, 8.626545752408477, 15.077315330820745, 14.407515022378812, 10.710746160680303, 12.647864134326667, 11.664486970459347, 10.785810146865035, 10.587233746797057, 8.445986032041889, 7.347280725328918, 7.946004424390734, 9.046082996418443, 8.44693538278046, 6.7396024328392, 6.323388199394318, 6.171112193881494, 13.63884349233672, 5.243777495434272, 4.4044727433705235, 3.5675859760318573, 3.5628651737417925, 9.23770957718589, 9.237709576956302, 7.679942138488749, 10.264866370680485, 2.7300845378266585, 2.730026591110091, 2.7300051911412258, 2.729118178643771, 2.728507751313579, 2.7282960623697528, 2.726568885787798, 2.726568885782741, 2.725911939835755, 2.7252890148299778, 2.720284604007364, 2.71698950650069, 2.7129406064828934, 10.054157116398851, 2.616467316657389, 7.579122641997085, 1.8900401101646844, 1.8900401101646844, 1.8900397996605247, 1.890038696236122, 1.8900344033131122, 6.79770753632412, 6.092483015034091, 3.5720468790547835, 3.5718080877930984, 9.83204334844769, 58.32892890162377, 8.304508219557349, 10.444808444840863, 10.928759689485274, 9.164136293577943, 21.570140352626435, 5.250408584896436, 12.276915344000166, 5.886720165436074, 10.022618381293181, 4.412537712926643, 6.822053626335497, 6.091738353690465, 5.100636534270405, 4.364271739386626, 9.910204476932922, 6.463735992639604, 11.025657845911786, 12.014406317295453, 5.603060452788867, 8.547266083132252, 10.067957282799847, 11.161787846656912, 8.60573441330252, 7.38051075307484, 7.859534488573297, 7.168547062792517, 8.08692526070862, 7.425933218505618, 7.096921322810177, 6.926207237067801], \"Term\": [\"case\", \"study\", \"case study\", \"method\", \"organization\", \"data\", \"new method\", \"ops\", \"ai\", \"methodology\", \"working\", \"research method\", \"presentation\", \"quant\", \"qual\", \"research ops\", \"mixed method\", \"mixed\", \"ethic\", \"stakeholder\", \"need\", \"new\", \"finding\", \"know\", \"practice\", \"research finding\", \"project\", \"technique\", \"way\", \"problem\", \"case\", \"case study\", \"organization\", \"study\", \"ethic research\", \"research project\", \"case study research\", \"study research\", \"research organization\", \"accessibility\", \"selling\", \"research case\", \"selling research\", \"research case study\", \"method case\", \"method case study\", \"promoting\", \"large organization\", \"study best\", \"heard\", \"advanced research\", \"advanced method\", \"inhouse research\", \"align\", \"research stuff\", \"application research\", \"topic case study\", \"topic case\", \"research need\", \"interested learning\", \"need\", \"market\", \"project\", \"best practice\", \"kind\", \"ethic\", \"practice\", \"advice\", \"practical\", \"challenge\", \"research method\", \"method\", \"field\", \"best\", \"user\", \"user research\", \"hear\", \"example\", \"talk\", \"application\", \"people\", \"different\", \"advanced\", \"ux\", \"analysis\", \"team\", \"researcher\", \"process\", \"work\", \"design\", \"methodology\", \"value research\", \"data visualization\", \"real life\", \"qual quant\", \"present\", \"matter\", \"mentoring\", \"fit\", \"role\", \"avoiding bias\", \"outcome\", \"research action\", \"bias research\", \"writing\", \"everyday\", \"researcher role\", \"worked\", \"qual research\", \"engage\", \"gdpr\", \"creation\", \"research program\", \"qual\", \"visualization\", \"manager\", \"quant\", \"ux researcher role\", \"accessible\", \"gdpr research\", \"midcareer\", \"research finding\", \"bias\", \"apply\", \"data\", \"stakeholder\", \"finding\", \"management\", \"value\", \"advocating\", \"ux research\", \"buyin\", \"skill\", \"research technique\", \"technique\", \"ux\", \"way\", \"life\", \"qualitative\", \"researcher\", \"emerging\", \"design\", \"people\", \"new\", \"team\", \"method\", \"real\", \"technology\", \"product\", \"impact\", \"workshop\", \"best\", \"work\", \"analysis\", \"failure\", \"ai\", \"leading research\", \"leading research team\", \"ops\", \"growing research\", \"listening\", \"challenge research\", \"research social\", \"usability testing\", \"empathy\", \"research question\", \"org research\", \"research ai\", \"today\", \"roi\", \"ops research\", \"research ops\", \"enterprise\", \"research infrastructure\", \"infrastructure\", \"skill set\", \"scaling research practice\", \"compelling way\", \"special consideration\", \"consideration\", \"article\", \"foresight\", \"latest tool\", \"make good\", \"world\", \"latest\", \"storytelling\", \"deep\", \"research topic\", \"research planning\", \"actionable\", \"methodology\", \"organizational\", \"org\", \"testing\", \"book\", \"question\", \"creative\", \"good\", \"based\", \"synthesis\", \"planning\", \"team\", \"way\", \"user\", \"new\", \"topic\", \"research team\", \"analysis\", \"design\", \"approach\", \"technique\", \"product\", \"tool\", \"practice\", \"insight\", \"technology\", \"ethic\", \"data\", \"ux\", \"experience\", \"qualitative\", \"want\", \"new method\", \"operation\", \"leadership research\", \"don know\", \"working product\", \"mixed method\", \"mixed\", \"know\", \"working\", \"large orgs\", \"research want\", \"like new\", \"participatory\", \"new interesting\", \"philosophy\", \"know don\", \"know don know\", \"presentation skill\", \"research working\", \"research operation\", \"ia\", \"non\", \"presentation\", \"planning research\", \"used\", \"research agile process\", \"agile process\", \"contact\", \"business strategy\", \"sharing insight\", \"content\", \"scaling research\", \"firm\", \"orgs\", \"problem\", \"method\", \"agile\", \"business\", \"strategy\", \"leadership\", \"new\", \"academic\", \"like\", \"thing\", \"different\", \"influence\", \"industry\", \"scaling\", \"research agile\", \"interesting\", \"research method\", \"think\", \"tool\", \"researcher\", \"method research\", \"insight\", \"product\", \"analysis\", \"impact\", \"want\", \"people\", \"field\", \"design\", \"topic\", \"team\", \"technique\"], \"Total\": [51.0, 58.0, 43.0, 127.0, 28.0, 46.0, 14.0, 20.0, 14.0, 25.0, 12.0, 23.0, 12.0, 16.0, 15.0, 15.0, 10.0, 10.0, 26.0, 22.0, 17.0, 61.0, 19.0, 9.0, 39.0, 11.0, 18.0, 38.0, 38.0, 16.0, 51.42409268804037, 43.70886083220716, 28.276052378994702, 58.25361999752884, 5.9898980386315515, 5.989833893244811, 5.132655493932924, 5.132655493932908, 5.132588825928453, 5.132590338553403, 5.132590157065694, 5.132488356293196, 4.275446702312903, 4.2753885458358045, 4.275379410317208, 4.275379410317209, 4.275351241749199, 3.4181963641444617, 3.4181963394901076, 3.418196198429821, 3.4181959709762357, 3.4181957409021093, 3.418195714167185, 3.418195417093998, 3.4181950888453527, 3.4181948571982996, 3.4181618457676533, 3.4181618457676533, 3.4181630401266134, 3.418156065520593, 17.097225125675457, 9.39106582880744, 18.77980813000662, 14.487342230156777, 5.981524660858719, 26.361297790396037, 39.96328173007933, 5.971273107581557, 19.591926057837433, 12.763472605202404, 23.78308178302577, 127.08351597146752, 23.810721275858146, 32.29089379045431, 30.509549075596013, 14.440451396125434, 10.20622780010929, 14.451389584820479, 14.432257249075304, 14.449206804794072, 30.5532745992871, 22.027161618580244, 17.833610468583075, 40.712895606427395, 46.51914265268909, 48.23035231905884, 37.3022172361498, 16.125115704401544, 25.437985066680053, 43.977816650338376, 25.352462484589473, 6.787522296871417, 5.090082163457774, 5.09007781603631, 5.090082190638722, 4.241355893184966, 4.241349700781733, 4.241266535513382, 4.241058378596949, 8.494095727145346, 3.3926256527341665, 3.392625594617174, 3.3926255775958083, 3.3926255635712788, 3.3926141014192397, 3.3926378865920155, 3.392643862555225, 3.392646445864874, 3.3925821697178518, 3.3925713345321373, 3.392680657950307, 3.392550429176496, 3.3923485018536574, 15.247884080849605, 6.796047553283971, 6.777870066486204, 16.089012530918573, 2.543897432874704, 2.543897440173629, 2.5438974326275314, 2.5438974136719885, 11.853140669892651, 15.259674202604852, 8.467354255798757, 46.524758496871264, 22.901018936745142, 19.448922595551903, 20.359986854164156, 15.245993339591347, 10.210485635627009, 20.33085719572093, 10.16555563360968, 21.955492592247438, 10.14721422397529, 38.00879388804379, 40.712895606427395, 38.06234426254424, 6.787144176477172, 27.1329637640499, 37.3022172361498, 16.12150158088585, 43.977816650338376, 30.5532745992871, 61.58289828750088, 48.23035231905884, 127.08351597146752, 18.684908803881772, 18.562829745251506, 36.310315795948114, 27.896111654240038, 18.628028107240546, 32.29089379045431, 25.437985066680053, 46.51914265268909, 6.724886567892024, 14.294664912750157, 5.0451891477246535, 5.0451891477246535, 20.20140447830092, 3.3656900664710587, 3.3656904765037146, 3.3657015556557686, 3.3657470477237563, 3.365779360567668, 3.3657537722315736, 3.36576681439318, 3.3656927377088746, 3.36577832543759, 3.3658256452652386, 3.3659149511139734, 3.3658878680945294, 15.163740403641718, 5.885258455736366, 2.5259403654249226, 2.5259403654249226, 2.525940402571839, 2.5259404064517184, 2.5259404197216884, 2.5259404229348004, 2.5259404229348004, 2.525940448679303, 2.5259404465258664, 2.5259404697611907, 2.5259405036913267, 9.270280449413995, 5.894015039154096, 6.737042273426963, 6.749233110190186, 5.045721848573208, 5.045474674161513, 5.06265098223779, 25.352462484589473, 5.062919047271357, 9.280713370801395, 9.270842358942224, 9.248009026398048, 13.470258647081604, 9.297198205733787, 14.365637802129969, 9.28239632401116, 13.515457260098273, 11.813043338326848, 48.23035231905884, 38.06234426254424, 30.509549075596013, 61.58289828750088, 28.705437466435924, 18.597749645508205, 46.51914265268909, 43.977816650338376, 27.90596413624611, 38.00879388804379, 36.310315795948114, 32.05181180008293, 39.96328173007933, 23.6000453942913, 18.562829745251506, 26.361297790396037, 46.524758496871264, 40.712895606427395, 16.086562792290177, 27.1329637640499, 22.83598722760693, 14.287119043702386, 5.886775808420081, 5.046688724584609, 4.206630216709259, 4.20657592840342, 10.937747647659794, 10.937747647663056, 9.24656080643557, 12.624365058080448, 3.366543973839973, 3.366544560138983, 3.366544725308329, 3.3665627961448568, 3.3665761755120003, 3.3665803904106166, 3.3666160910586678, 3.3666160910587712, 3.3665871790954895, 3.3665555757385683, 3.3667438906888227, 3.3666077743765936, 3.366689275787492, 12.626797893184964, 3.3665643836057937, 10.103481920633566, 2.526509823203706, 2.526509823203706, 2.5265098309896783, 2.526509859890539, 2.526509902346569, 9.275556385624677, 8.406181660784249, 5.046328896281732, 5.055358669741258, 16.049752824815066, 127.08351597146752, 14.333472267019177, 20.24588709243622, 21.95627029446056, 17.744590606409847, 61.58289828750088, 8.432871423462188, 28.731020292470603, 10.103711239073585, 22.027161618580244, 6.726160158532091, 12.677113502635954, 10.934331679509185, 8.433927213086513, 6.747021157358388, 23.78308178302577, 12.677011453621787, 32.05181180008293, 37.3022172361498, 10.13107271172844, 23.6000453942913, 36.310315795948114, 46.51914265268909, 27.896111654240038, 22.83598722760693, 30.5532745992871, 23.810721275858146, 43.977816650338376, 28.705437466435924, 48.23035231905884, 38.00879388804379], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2549, 1.2527, 1.2406, 1.2059, 1.155, 1.1549, 1.1356, 1.1356, 1.1349, 1.1349, 1.1349, 1.1319, 1.1075, 1.1067, 1.1066, 1.1066, 1.1062, 1.063, 1.063, 1.063, 1.063, 1.063, 1.063, 1.063, 1.063, 1.063, 1.0624, 1.0624, 1.0618, 1.0616, 1.0475, 0.9529, 0.8712, 0.8761, 0.9859, 0.766, 0.6789, 0.9719, 0.7317, 0.8095, 0.6183, 0.2202, 0.6114, 0.4188, 0.3974, 0.6673, 0.7717, 0.6225, 0.6127, 0.6031, 0.1824, 0.3513, 0.4468, -0.016, -0.1172, -0.1684, -0.0349, 0.4693, 0.0919, -0.4101, 0.0744, 1.2479, 1.2133, 1.2132, 1.2125, 1.1843, 1.1843, 1.1821, 1.1757, 1.1446, 1.1394, 1.1394, 1.1394, 1.1394, 1.139, 1.1389, 1.1387, 1.1387, 1.1379, 1.1375, 1.1374, 1.1368, 1.1299, 1.1074, 1.0989, 1.0828, 1.0626, 1.0596, 1.0596, 1.0596, 1.0596, 1.0385, 1.0313, 1.0218, 0.9529, 0.9362, 0.8843, 0.852, 0.8592, 0.9317, 0.7358, 0.8748, 0.6468, 0.8476, 0.4891, 0.4345, 0.4451, 0.9221, 0.4625, 0.325, 0.6247, 0.148, 0.2581, -0.114, -0.0367, -0.6056, 0.4814, 0.4838, 0.0728, 0.2245, 0.4814, 0.0887, 0.1771, -0.338, 1.3511, 1.3186, 1.3177, 1.3177, 1.257, 1.2429, 1.2429, 1.2427, 1.2419, 1.2413, 1.2407, 1.2401, 1.2399, 1.2399, 1.2386, 1.2351, 1.234, 1.1729, 1.1643, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1622, 1.1619, 1.1614, 1.1405, 1.1342, 1.1067, 1.1065, 1.1029, 0.9543, 1.0988, 1.0333, 1.0328, 1.0284, 0.9765, 1.0231, 0.959, 1.0043, 0.8756, 0.9077, 0.4424, 0.486, 0.5497, 0.3133, 0.5513, 0.6844, 0.3259, 0.3366, 0.495, 0.3522, 0.317, 0.3634, 0.1243, 0.425, 0.5257, 0.2533, -0.1851, -0.1202, 0.5826, -0.0039, 0.1441, 1.4482, 1.379, 1.3586, 1.3299, 1.3286, 1.3257, 1.3257, 1.309, 1.2878, 1.2851, 1.2851, 1.2851, 1.2848, 1.2845, 1.2844, 1.2838, 1.2838, 1.2836, 1.2834, 1.2815, 1.2803, 1.2788, 1.2668, 1.2426, 1.2072, 1.2044, 1.2044, 1.2044, 1.2044, 1.2044, 1.1839, 1.1728, 1.1491, 1.1473, 1.0046, 0.7159, 0.9489, 0.8328, 0.797, 0.8339, 0.4456, 1.0208, 0.6444, 0.9545, 0.7072, 1.0731, 0.875, 0.9097, 0.9918, 1.059, 0.6193, 0.8211, 0.4275, 0.3617, 0.9024, 0.479, 0.2119, 0.0673, 0.3186, 0.3652, 0.1369, 0.2942, -0.1988, 0.1426, -0.4217, -0.2078], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5393, -3.704, -4.1517, -3.4635, -5.7892, -5.7893, -5.963, -5.963, -5.9637, -5.9637, -5.9637, -5.9667, -6.1739, -6.1747, -6.1748, -6.1748, -6.1752, -6.4421, -6.4421, -6.4421, -6.4421, -6.4421, -6.4421, -6.4421, -6.4421, -6.4421, -6.4427, -6.4427, -6.4433, -6.4436, -4.8478, -5.5415, -4.9302, -5.1849, -5.9596, -4.6963, -4.3674, -5.9754, -5.0274, -5.3781, -4.9469, -3.6692, -4.9527, -4.8407, -4.9188, -5.3969, -5.6395, -5.4409, -5.4521, -5.4605, -5.1323, -5.2907, -5.4064, -5.0437, -5.0116, -5.0267, -5.1501, -5.4846, -5.4061, -5.3607, -5.4269, -5.5713, -5.8937, -5.8938, -5.8944, -6.1051, -6.1051, -6.1073, -6.1138, -5.4503, -6.3732, -6.3732, -6.3732, -6.3732, -6.3737, -6.3737, -6.3739, -6.374, -6.3747, -6.3752, -6.3752, -6.3758, -6.3828, -4.9024, -5.719, -5.7377, -4.8935, -6.741, -6.741, -6.741, -6.741, -5.2232, -4.9777, -5.5762, -3.9414, -4.6668, -4.8821, -4.8687, -5.1507, -5.4791, -4.9863, -5.5405, -4.9984, -5.5695, -4.6073, -4.5932, -4.6499, -5.8971, -4.971, -4.7902, -5.3294, -4.8025, -5.0566, -4.7279, -4.8949, -4.495, -5.3251, -5.3292, -5.0693, -5.1812, -5.3281, -5.1707, -5.3209, -5.2324, -5.4773, -4.7557, -5.7981, -5.7981, -4.4714, -6.2777, -6.2777, -6.2779, -6.2787, -6.2793, -6.2799, -6.2804, -6.2807, -6.2807, -6.2819, -6.2854, -6.2865, -4.8424, -5.7975, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -6.6454, -5.3455, -5.7989, -5.6861, -5.6906, -6.009, -6.0092, -6.0095, -4.5471, -6.0135, -5.473, -5.4746, -5.4815, -5.1573, -5.4815, -5.1104, -5.5018, -5.2548, -5.3574, -4.4159, -4.6091, -4.7665, -4.3005, -4.8259, -5.1268, -4.5685, -4.6139, -4.9104, -4.7442, -4.8251, -4.9034, -4.922, -5.148, -5.2873, -5.209, -5.0793, -5.1479, -5.3737, -5.4374, -5.4618, -4.6266, -5.5825, -5.7569, -5.9677, -5.969, -5.0163, -5.0163, -5.201, -4.9108, -6.2352, -6.2353, -6.2353, -6.2356, -6.2358, -6.2359, -6.2365, -6.2365, -6.2368, -6.237, -6.2388, -6.24, -6.2415, -4.9316, -6.2777, -5.2142, -6.603, -6.603, -6.603, -6.603, -6.603, -5.323, -5.4325, -5.9664, -5.9665, -4.9539, -3.1735, -5.1228, -4.8935, -4.8482, -5.0243, -4.1683, -5.5813, -4.7318, -5.4669, -4.9347, -5.7551, -5.3194, -5.4326, -5.6102, -5.7661, -4.946, -5.3734, -4.8393, -4.7535, -5.5163, -5.094, -4.9302, -4.8271, -5.0871, -5.2407, -5.1778, -5.2699, -5.1493, -5.2346, -5.2799, -5.3043]}, \"token.table\": {\"Topic\": [2, 4, 1, 2, 1, 3, 1, 2, 3, 4, 1, 1, 1, 3, 1, 2, 1, 2, 3, 4, 4, 2, 3, 1, 1, 2, 3, 4, 1, 2, 4, 1, 2, 4, 1, 2, 3, 4, 3, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 1, 1, 1, 1, 3, 3, 3, 3, 4, 1, 2, 4, 2, 1, 3, 2, 3, 4, 2, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 3, 2, 3, 4, 1, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 4, 2, 3, 4, 3, 4, 2, 3, 2, 2, 1, 3, 4, 3, 1, 3, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 3, 1, 1, 2, 3, 4, 1, 1, 3, 4, 1, 2, 3, 4, 4, 4, 1, 4, 2, 3, 3, 1, 3, 4, 4, 3, 3, 1, 2, 4, 1, 2, 3, 4, 4, 3, 3, 1, 2, 3, 4, 2, 4, 1, 2, 4, 2, 2, 1, 2, 3, 4, 1, 1, 1, 2, 4, 1, 3, 4, 2, 2, 4, 2, 4, 1, 2, 3, 1, 2, 3, 4, 4, 4, 4, 4, 1, 2, 3, 3, 1, 2, 3, 3, 1, 1, 3, 2, 4, 2, 4, 1, 2, 3, 4, 4, 1, 3, 4, 4, 1, 2, 4, 1, 2, 3, 4, 2, 2, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 4, 2, 2, 1, 2, 3, 4, 2, 4, 2, 3, 4, 1, 2, 3, 2, 2, 1, 2, 3, 4, 4, 3, 1, 1, 2, 3, 3, 1, 2, 4, 1, 4, 1, 2, 3, 1, 3, 4, 2, 1, 3, 3, 1, 1, 2, 3, 4, 2, 3, 4, 3, 4, 4, 4, 1, 2, 3, 4, 2, 3, 1, 2, 2, 3, 4, 3, 4, 3, 1, 1, 4, 2, 3, 4, 3, 3, 1, 2, 4, 2, 3, 1, 2, 3, 4, 1, 2, 1, 1, 1, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 2, 3, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 3, 1, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 2, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 2, 4, 4, 1, 2, 3, 4, 1, 2, 3, 2], \"Freq\": [0.3557507104464216, 0.592917850744036, 0.7793335793729803, 0.7861952169988008, 0.19752497328148436, 0.7900998931259374, 0.4485911596024459, 0.22429557980122294, 0.22429557980122294, 0.11214778990061147, 0.8776559996556131, 0.8776559405817803, 0.669873899239563, 0.16746847480989074, 0.2938156035920788, 0.6855697417148505, 0.06976676560786776, 0.20930029682360332, 0.13953353121573553, 0.5581341248629421, 0.7916058673636691, 0.06995616938932565, 0.9094302020612335, 0.8776560827965976, 0.25795832243925343, 0.19346874182944007, 0.3224479030490668, 0.23646179556931565, 0.48445565867861234, 0.13841590247960353, 0.34603975619900884, 0.8776562265554778, 0.7086038706708153, 0.23620129022360511, 0.21500780158341865, 0.21500780158341865, 0.39418096956960086, 0.1791731679861822, 0.7917843039592273, 0.884270858938491, 0.10773080194962786, 0.21546160389925573, 0.6463848116977672, 0.43355876399242366, 0.2787163482808438, 0.18581089885389585, 0.09290544942694792, 0.6902577326560321, 0.0690257732656032, 0.20707731979680963, 0.06553219857271253, 0.7208541842998378, 0.06553219857271253, 0.13106439714542506, 0.884270882178351, 0.6487882940937076, 0.3243941470468538, 0.14817824411955688, 0.14817824411955688, 0.19757099215940918, 0.49392748039852297, 0.7916058558689535, 0.0983714059557914, 0.5902284357347484, 0.2951142178673742, 0.9917530350877927, 0.9837822167242385, 0.7793236862922548, 0.6267886685273405, 0.31339433426367025, 0.8913446276776267, 0.7917843130363157, 0.7917843120291298, 0.7916058649241688, 0.1078102443051079, 0.1078102443051079, 0.7546717101357553, 0.8842904660162167, 0.32267785773888674, 0.6453557154777735, 0.6663118950329363, 0.19344538888052992, 0.12896359258701995, 0.7858419317307711, 0.1481649816614233, 0.7408249083071166, 0.18190989479097858, 0.2956035790353402, 0.3183423158842125, 0.18190989479097858, 0.40858646047289016, 0.04539849560809891, 0.09079699121619782, 0.45398495608098904, 0.9508798715207963, 0.24811584578092422, 0.49623169156184843, 0.062028961445231054, 0.18608688433569318, 0.8913307992850974, 0.8842850169320681, 0.6796642883374471, 0.16991607208436177, 0.6069503909564395, 0.3034751954782198, 0.07586879886955494, 0.8347387497671491, 0.8842676702563063, 0.5535799829521639, 0.13839499573804098, 0.20759249360706147, 0.13839499573804098, 0.3108184181145493, 0.12432736724581972, 0.435145785360369, 0.12432736724581972, 0.8922083576319346, 0.5039746533074109, 0.16799155110247027, 0.29398521442932296, 0.6170007588360951, 0.25708364951503965, 0.10283345980601585, 0.1981638574403714, 0.7926554297614856, 0.9431607968865787, 0.791784304634246, 0.8842565223372525, 0.7861952193309332, 0.2784422143378087, 0.6264949822600696, 0.06961055358445217, 0.8913476703888881, 0.5878763552520111, 0.3919175701680074, 0.8776558821808054, 0.8911046967909769, 0.1433891593774154, 0.32262560859918465, 0.2150837390661231, 0.32262560859918465, 0.23664693065785122, 0.07888231021928374, 0.07888231021928374, 0.5521761715349861, 0.29734647300406797, 0.5946929460081359, 0.7917843300562454, 0.8776560065200729, 0.04237279985240595, 0.21186399926202973, 0.3389823988192476, 0.3813551986716535, 0.8776661868255254, 0.14821355627577767, 0.14821355627577767, 0.5928542251031107, 0.8359072784098813, 0.16718145568197626, 0.10814831816214349, 0.8651865452971479, 0.8911024954605438, 0.8911024954605163, 0.8776558396319247, 0.8911215844236001, 0.1696636322365949, 0.6786545289463796, 0.7917842973508736, 0.2817760133724279, 0.16906560802345674, 0.5071968240703703, 0.7925989135240827, 0.7928344969591424, 0.7928344969591424, 0.14733737401156022, 0.5893494960462409, 0.14733737401156022, 0.20883351648922785, 0.17402793040768988, 0.17402793040768988, 0.4176670329784557, 0.8911213855105523, 0.8913475617984947, 0.791784286715093, 0.19646378107468643, 0.5893913432240593, 0.09823189053734321, 0.09823189053734321, 0.7376948733087929, 0.14753897466175858, 0.7453893016623568, 0.10648418595176526, 0.10648418595176526, 0.9430960147574606, 0.9431145075431631, 0.3540978517631137, 0.1416391407052455, 0.0472130469017485, 0.45639278671690214, 0.9355894801634047, 0.9355894801634045, 0.09870623066818285, 0.3948249226727314, 0.5922373840090971, 0.31555120157905014, 0.5916585029607191, 0.07888780039476254, 0.7861952251891715, 0.09142650134314066, 0.822838512088266, 0.09142650134316793, 0.8228385120885113, 0.8188463272309462, 0.11697804674727802, 0.05848902337363901, 0.09742964632792843, 0.227335841431833, 0.32476548775976144, 0.35724203653573755, 0.8911130607474669, 0.9799036430770873, 0.8910831247704851, 0.849361375856765, 0.04950150872302652, 0.09900301744605304, 0.8415256482914509, 0.8912952889599192, 0.10775033772146865, 0.2155006754429373, 0.6465020263288118, 0.8913469629560385, 0.9902372376704263, 0.1975145149790508, 0.7900580599162031, 0.19780990139937624, 0.791239605597505, 0.8842708740863938, 0.89111660220192, 0.3272971598348195, 0.3272971598348195, 0.0654594319669639, 0.2618377278678556, 0.8911119450898051, 0.25395657275434225, 0.5925653364267985, 0.16930438183622815, 0.8911161820071354, 0.5614557735429808, 0.20416573583381123, 0.20416573583381123, 0.5505053400917564, 0.100091880016683, 0.2752526700458782, 0.07506891001251224, 0.9430946378320249, 0.15839328521124552, 0.7919664260562276, 0.8911101481726721, 0.2492250219464729, 0.06230625548661822, 0.06230625548661822, 0.6230625548661822, 0.43410541222282617, 0.12403011777795032, 0.24806023555590065, 0.1860451766669255, 0.11016153157352496, 0.2754038289338124, 0.33048459472057484, 0.2754038289338124, 0.6922328444468201, 0.21299472136825232, 0.10649736068412616, 0.9355956443858066, 0.7869944404332962, 0.19674861010832406, 0.7858419275343892, 0.8842821927138462, 0.25798877191863284, 0.40541092730070877, 0.2211332330731139, 0.11056661653655694, 0.7458506217792648, 0.2486168739264216, 0.2227128727517021, 0.5939009940045389, 0.14847524850113472, 0.32111475966923136, 0.42815301289230845, 0.21407650644615422, 0.7858426029162039, 0.8842708785229276, 0.11856872542702869, 0.11856872542702869, 0.11856872542702869, 0.5928436271351435, 0.7916058673636691, 0.8913242970658103, 0.779349064688165, 0.9355874810246122, 0.7592924314869798, 0.2530974771623266, 0.7917843300562454, 0.5045603471188718, 0.04204669559323932, 0.4204669559323932, 0.8776643959876401, 0.891068669730685, 0.06594678973532417, 0.13189357947064834, 0.7254146870885658, 0.7793338090503334, 0.7927896299796102, 0.19819740749490256, 0.8843431028270624, 0.8347476890200375, 0.8913273454271892, 0.8913325800965614, 0.8776561670777496, 0.16130983894196954, 0.3226196778839391, 0.4839295168259087, 0.053769946313989854, 0.5912952922412462, 0.19709843074708208, 0.19709843074708208, 0.7927507936512772, 0.1981876984128193, 0.891121429230733, 0.8911185134206044, 0.26808057914340117, 0.3485047528864215, 0.053616115828680234, 0.3216966949720814, 0.8842661126654482, 0.8912881173682444, 0.1177288356668986, 0.8241018496682901, 0.09145506367563266, 0.36582025470253066, 0.548730382053796, 0.23792014980240284, 0.7137604494072085, 0.7917843171959364, 0.7793336069301124, 0.9355747547586329, 0.791605842566634, 0.501013582536706, 0.18218675728607492, 0.2732801359291124, 0.7917843184121282, 0.7917843120291298, 0.13099853802515485, 0.6549926901257743, 0.17466471736687314, 0.1484330896874906, 0.742165448437453, 0.1366352281041504, 0.227725380173584, 0.1366352281041504, 0.5009958363818848, 0.9441473337164822, 0.05149894547544449, 0.8776558459621749, 0.7793236862922572, 0.2959574302979162, 0.5919148605958324, 0.1479787151489581, 0.4850246139042803, 0.207867691673263, 0.207867691673263, 0.22807214691760005, 0.2488059784555637, 0.37320896768334555, 0.1451368207657455, 0.0526194018650281, 0.4209552149202248, 0.34202611212268264, 0.18416790652759835, 0.4309687752238558, 0.3770976783208738, 0.16161329070894592, 0.3235951905822603, 0.6471903811645205, 0.1979470664467823, 0.1979470664467823, 0.593841199340347, 0.15776589043221267, 0.236648835648319, 0.07888294521610634, 0.473297671296638, 0.8913117660209608, 0.09359845298955105, 0.21839639030895247, 0.3431943276283539, 0.3431943276283539, 0.17418302737403993, 0.17418302737403993, 0.41803926569769584, 0.2438562383236559, 0.877664702657243, 0.877664702657243, 0.8913240229430915, 0.09897577962284237, 0.09897577962284237, 0.791806236982739, 0.42609610413411336, 0.06555324678986359, 0.39331948073918155, 0.13110649357972717, 0.5539993024142241, 0.27699965120711206, 0.13849982560355603, 0.2701846635114653, 0.39299587419849497, 0.19649793709924748, 0.12281121068702969, 0.1475589529314787, 0.5410494940820886, 0.19674527057530494, 0.1475589529314787, 0.7861952192545442, 0.13118200667229254, 0.5903190300253165, 0.19677301000843883, 0.13118200667229254, 0.8839749967032283, 0.14714435002986143, 0.7357217501493072, 0.2627431842642856, 0.1313715921321428, 0.2627431842642856, 0.30653371497499987, 0.1313634274733918, 0.39409028242017535, 0.36781759692549704, 0.07881805648403507, 0.31449031749290557, 0.31449031749290557, 0.19655644843306597, 0.19655644843306597, 0.8842654393464869, 0.1584238091023726, 0.7921190455118631, 0.9508921431778781, 0.16104764190440143, 0.4294603784117371, 0.21473018920586856, 0.16104764190440143, 0.10787160166909657, 0.10787160166909657, 0.7551012116836759, 0.8842738697410364], \"Term\": [\"academic\", \"academic\", \"accessibility\", \"accessible\", \"actionable\", \"actionable\", \"advanced\", \"advanced\", \"advanced\", \"advanced\", \"advanced method\", \"advanced research\", \"advice\", \"advice\", \"advocating\", \"advocating\", \"agile\", \"agile\", \"agile\", \"agile\", \"agile process\", \"ai\", \"ai\", \"align\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"application\", \"application\", \"application\", \"application research\", \"apply\", \"apply\", \"approach\", \"approach\", \"approach\", \"approach\", \"article\", \"avoiding bias\", \"based\", \"based\", \"based\", \"best\", \"best\", \"best\", \"best\", \"best practice\", \"best practice\", \"best practice\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias research\", \"book\", \"book\", \"business\", \"business\", \"business\", \"business\", \"business strategy\", \"buyin\", \"buyin\", \"buyin\", \"case\", \"case study\", \"case study research\", \"challenge\", \"challenge\", \"challenge research\", \"compelling way\", \"consideration\", \"contact\", \"content\", \"content\", \"content\", \"creation\", \"creative\", \"creative\", \"data\", \"data\", \"data\", \"data visualization\", \"deep\", \"deep\", \"design\", \"design\", \"design\", \"design\", \"different\", \"different\", \"different\", \"different\", \"don know\", \"emerging\", \"emerging\", \"emerging\", \"emerging\", \"empathy\", \"engage\", \"enterprise\", \"enterprise\", \"ethic\", \"ethic\", \"ethic\", \"ethic research\", \"everyday\", \"example\", \"example\", \"example\", \"example\", \"experience\", \"experience\", \"experience\", \"experience\", \"failure\", \"field\", \"field\", \"field\", \"finding\", \"finding\", \"finding\", \"firm\", \"firm\", \"fit\", \"foresight\", \"gdpr\", \"gdpr research\", \"good\", \"good\", \"good\", \"growing research\", \"hear\", \"hear\", \"heard\", \"ia\", \"impact\", \"impact\", \"impact\", \"impact\", \"industry\", \"industry\", \"industry\", \"industry\", \"influence\", \"influence\", \"infrastructure\", \"inhouse research\", \"insight\", \"insight\", \"insight\", \"insight\", \"interested learning\", \"interesting\", \"interesting\", \"interesting\", \"kind\", \"kind\", \"know\", \"know\", \"know don\", \"know don know\", \"large organization\", \"large orgs\", \"latest\", \"latest\", \"latest tool\", \"leadership\", \"leadership\", \"leadership\", \"leadership research\", \"leading research\", \"leading research team\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like new\", \"listening\", \"make good\", \"management\", \"management\", \"management\", \"management\", \"manager\", \"manager\", \"market\", \"market\", \"market\", \"matter\", \"mentoring\", \"method\", \"method\", \"method\", \"method\", \"method case\", \"method case study\", \"method research\", \"method research\", \"method research\", \"methodology\", \"methodology\", \"methodology\", \"midcareer\", \"mixed\", \"mixed\", \"mixed method\", \"mixed method\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new interesting\", \"new method\", \"non\", \"operation\", \"ops\", \"ops\", \"ops\", \"ops research\", \"org\", \"org\", \"org\", \"org research\", \"organization\", \"organizational\", \"organizational\", \"orgs\", \"orgs\", \"outcome\", \"participatory\", \"people\", \"people\", \"people\", \"people\", \"philosophy\", \"planning\", \"planning\", \"planning\", \"planning research\", \"practical\", \"practical\", \"practical\", \"practice\", \"practice\", \"practice\", \"practice\", \"present\", \"presentation\", \"presentation\", \"presentation skill\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"product\", \"product\", \"product\", \"product\", \"project\", \"project\", \"project\", \"promoting\", \"qual\", \"qual\", \"qual quant\", \"qual research\", \"qualitative\", \"qualitative\", \"qualitative\", \"qualitative\", \"quant\", \"quant\", \"question\", \"question\", \"question\", \"real\", \"real\", \"real\", \"real life\", \"research action\", \"research agile\", \"research agile\", \"research agile\", \"research agile\", \"research agile process\", \"research ai\", \"research case\", \"research case study\", \"research finding\", \"research finding\", \"research infrastructure\", \"research method\", \"research method\", \"research method\", \"research need\", \"research operation\", \"research ops\", \"research ops\", \"research ops\", \"research organization\", \"research planning\", \"research planning\", \"research program\", \"research project\", \"research question\", \"research social\", \"research stuff\", \"research team\", \"research team\", \"research team\", \"research team\", \"research technique\", \"research technique\", \"research technique\", \"research topic\", \"research topic\", \"research want\", \"research working\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher role\", \"roi\", \"role\", \"role\", \"scaling\", \"scaling\", \"scaling\", \"scaling research\", \"scaling research\", \"scaling research practice\", \"selling\", \"selling research\", \"sharing insight\", \"skill\", \"skill\", \"skill\", \"skill set\", \"special consideration\", \"stakeholder\", \"stakeholder\", \"stakeholder\", \"storytelling\", \"storytelling\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"study\", \"study\", \"study best\", \"study research\", \"synthesis\", \"synthesis\", \"synthesis\", \"talk\", \"talk\", \"talk\", \"team\", \"team\", \"team\", \"team\", \"technique\", \"technique\", \"technique\", \"technique\", \"technology\", \"technology\", \"technology\", \"testing\", \"testing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"today\", \"tool\", \"tool\", \"tool\", \"tool\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic case\", \"topic case study\", \"usability testing\", \"used\", \"used\", \"used\", \"user\", \"user\", \"user\", \"user\", \"user research\", \"user research\", \"user research\", \"ux\", \"ux\", \"ux\", \"ux\", \"ux research\", \"ux research\", \"ux research\", \"ux research\", \"ux researcher role\", \"value\", \"value\", \"value\", \"value\", \"value research\", \"visualization\", \"visualization\", \"want\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"way\", \"work\", \"work\", \"work\", \"work\", \"worked\", \"working\", \"working\", \"working product\", \"workshop\", \"workshop\", \"workshop\", \"workshop\", \"world\", \"world\", \"world\", \"writing\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el328611123078614489888181427\", ldavis_el328611123078614489888181427_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el328611123078614489888181427\", ldavis_el328611123078614489888181427_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el328611123078614489888181427\", ldavis_el328611123078614489888181427_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3     -0.148860 -0.008165       1        1  28.152875\n",
       "1      0.076225  0.007506       2        1  26.018038\n",
       "2      0.044807 -0.106360       3        1  23.396824\n",
       "0      0.027827  0.107019       4        1  22.432263, topic_info=     Category        Freq          Term       Total  loglift  logprob\n",
       "132   Default   51.000000          case   51.000000  30.0000  30.0000\n",
       "989   Default   58.000000         study   58.000000  29.0000  29.0000\n",
       "134   Default   43.000000    case study   43.000000  28.0000  28.0000\n",
       "582   Default  127.000000        method  127.000000  27.0000  27.0000\n",
       "663   Default   28.000000  organization   28.000000  26.0000  26.0000\n",
       "...       ...         ...           ...         ...      ...      ...\n",
       "349    Topic4    7.168547         field   23.810721   0.2942  -5.2699\n",
       "244    Topic4    8.086925        design   43.977817  -0.1988  -5.1493\n",
       "1041   Topic4    7.425933         topic   28.705437   0.1426  -5.2346\n",
       "1012   Topic4    7.096921          team   48.230352  -0.4217  -5.2799\n",
       "1016   Topic4    6.926207     technique   38.008794  -0.2078  -5.3043\n",
       "\n",
       "[289 rows x 6 columns], token_table=      Topic      Freq           Term\n",
       "term                                \n",
       "4         2  0.355751       academic\n",
       "4         4  0.592918       academic\n",
       "7         1  0.779334  accessibility\n",
       "9         2  0.786195     accessible\n",
       "12        1  0.197525     actionable\n",
       "...     ...       ...            ...\n",
       "1117      4  0.161048       workshop\n",
       "1119      1  0.107872          world\n",
       "1119      2  0.107872          world\n",
       "1119      3  0.755101          world\n",
       "1120      2  0.884274        writing\n",
       "\n",
       "[453 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 2, 3, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda21, ideal_topics_matrix, ideal_topics_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4) - considerable overlap between 2 & 3\n",
    "1. case study\n",
    "2. finding\n",
    "3. ops\n",
    "4. new method\n",
    "\n",
    "#### Topic groups (6)\n",
    "1. case study\n",
    "2. ops\n",
    "3. stakeholder\n",
    "4. new method\n",
    "5. user research\n",
    "6. quantitative\n",
    "\n",
    "#### Topic groups (8)\n",
    "1. case study\n",
    "2. ops\n",
    "3. user research\n",
    "4. working\n",
    "5. quant\n",
    "6. mixed method\n",
    "7. ethic\n",
    "8. qualitative data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 topics, 2.Finding/Data 3.ops/ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W21 = lda21.transform(ideal_topics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column21 = datadf.ideal_topics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix21, count_vect21 = nlp.create_wordcount_matrix(datadf.ideal_topics.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA21a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA21a.fit(word_count_matrix21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA21a.transform(word_count_matrix21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column21, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. If attending a conference about research, who might you be excited to see there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['nan', 'Nan', 'NaN', 'NAN', 'research', 'conference', 'make', 'researcher', 'people', 'like', 'event', 'don']\n",
    "\n",
    "words_to_stop = nlp.set_stop_words(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'Nan',\n",
       " 'NaN',\n",
       " 'NAN',\n",
       " 'research',\n",
       " 'conference',\n",
       " 'make',\n",
       " 'researcher',\n",
       " 'people',\n",
       " 'like',\n",
       " 'event',\n",
       " 'don',\n",
       " 'am',\n",
       " 'however',\n",
       " 'whatever',\n",
       " 'bottom',\n",
       " 'namely',\n",
       " 'most',\n",
       " 'whole',\n",
       " 'since',\n",
       " 'among',\n",
       " 'than',\n",
       " 'anyone',\n",
       " 'hers',\n",
       " 'every',\n",
       " 'yet',\n",
       " 'few',\n",
       " 'sixty',\n",
       " 'together',\n",
       " 'serious',\n",
       " 'latter',\n",
       " 'find',\n",
       " 'cant',\n",
       " 'until',\n",
       " 'none',\n",
       " 'up',\n",
       " 'he',\n",
       " 'meanwhile',\n",
       " 'amoungst',\n",
       " 'keep',\n",
       " 'thin',\n",
       " 're',\n",
       " 'ourselves',\n",
       " 'yourself',\n",
       " 'via',\n",
       " 'herself',\n",
       " 'only',\n",
       " 'either',\n",
       " 'cannot',\n",
       " 'anyhow',\n",
       " 'herein',\n",
       " 'both',\n",
       " 'formerly',\n",
       " 'anyway',\n",
       " 'how',\n",
       " 'never',\n",
       " 'when',\n",
       " 'though',\n",
       " 'six',\n",
       " 'him',\n",
       " 'bill',\n",
       " 'often',\n",
       " 'whose',\n",
       " 'below',\n",
       " 'from',\n",
       " 'full',\n",
       " 'hereby',\n",
       " 'nor',\n",
       " 'everywhere',\n",
       " 'even',\n",
       " 'whoever',\n",
       " 'name',\n",
       " 'some',\n",
       " 'everyone',\n",
       " 'afterwards',\n",
       " 'nowhere',\n",
       " 'its',\n",
       " 'was',\n",
       " 'almost',\n",
       " 'perhaps',\n",
       " 'after',\n",
       " 'at',\n",
       " 'nothing',\n",
       " 'then',\n",
       " 'therefore',\n",
       " 'former',\n",
       " 'amount',\n",
       " 'detail',\n",
       " 'while',\n",
       " 'although',\n",
       " 'but',\n",
       " 'any',\n",
       " 'next',\n",
       " 'sometime',\n",
       " 'to',\n",
       " 'is',\n",
       " 'fifteen',\n",
       " 'well',\n",
       " 'are',\n",
       " 'could',\n",
       " 'side',\n",
       " 'now',\n",
       " 'whereafter',\n",
       " 'again',\n",
       " 'own',\n",
       " 'which',\n",
       " 'a',\n",
       " 'why',\n",
       " 'several',\n",
       " 'found',\n",
       " 'un',\n",
       " 'front',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'she',\n",
       " 'off',\n",
       " 'eight',\n",
       " 'around',\n",
       " 'one',\n",
       " 'co',\n",
       " 'those',\n",
       " 'whence',\n",
       " 'must',\n",
       " 'otherwise',\n",
       " 'himself',\n",
       " 'will',\n",
       " 'get',\n",
       " 'very',\n",
       " 'hereafter',\n",
       " 'system',\n",
       " 'first',\n",
       " 'beforehand',\n",
       " 'each',\n",
       " 'ours',\n",
       " 'our',\n",
       " 'ever',\n",
       " 'whether',\n",
       " 'eleven',\n",
       " 'put',\n",
       " 'towards',\n",
       " 'nobody',\n",
       " 'i',\n",
       " 'there',\n",
       " 'because',\n",
       " 'as',\n",
       " 'ten',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'becoming',\n",
       " 'over',\n",
       " 'during',\n",
       " 'still',\n",
       " 'seeming',\n",
       " 'alone',\n",
       " 'under',\n",
       " 'hundred',\n",
       " 'same',\n",
       " 'thick',\n",
       " 'third',\n",
       " 'an',\n",
       " 'so',\n",
       " 'them',\n",
       " 'rather',\n",
       " 'something',\n",
       " 'therein',\n",
       " 'back',\n",
       " 'thereupon',\n",
       " 'their',\n",
       " 'within',\n",
       " 'see',\n",
       " 'take',\n",
       " 'once',\n",
       " 'please',\n",
       " 'hereupon',\n",
       " 'wherein',\n",
       " 'out',\n",
       " 'mill',\n",
       " 'has',\n",
       " 'mostly',\n",
       " 'might',\n",
       " 'many',\n",
       " 'always',\n",
       " 'toward',\n",
       " 'someone',\n",
       " 'what',\n",
       " 'and',\n",
       " 'noone',\n",
       " 'made',\n",
       " 'ie',\n",
       " 'us',\n",
       " 'this',\n",
       " 'empty',\n",
       " 'three',\n",
       " 'seem',\n",
       " 'across',\n",
       " 'hasnt',\n",
       " 'interest',\n",
       " 'moreover',\n",
       " 'above',\n",
       " 'ltd',\n",
       " 'or',\n",
       " 'against',\n",
       " 'they',\n",
       " 'yours',\n",
       " 'fill',\n",
       " 'been',\n",
       " 'with',\n",
       " 'on',\n",
       " 'enough',\n",
       " 'whom',\n",
       " 'two',\n",
       " 'all',\n",
       " 'amongst',\n",
       " 'where',\n",
       " 'becomes',\n",
       " 'else',\n",
       " 'wherever',\n",
       " 'besides',\n",
       " 'much',\n",
       " 'before',\n",
       " 'who',\n",
       " 'through',\n",
       " 'inc',\n",
       " 'whereupon',\n",
       " 'if',\n",
       " 'beside',\n",
       " 'further',\n",
       " 'whereby',\n",
       " 'de',\n",
       " 'least',\n",
       " 'became',\n",
       " 'behind',\n",
       " 'about',\n",
       " 'of',\n",
       " 'elsewhere',\n",
       " 'being',\n",
       " 'upon',\n",
       " 'indeed',\n",
       " 'along',\n",
       " 'top',\n",
       " 'also',\n",
       " 'somewhere',\n",
       " 'fire',\n",
       " 'more',\n",
       " 'thereby',\n",
       " 'become',\n",
       " 'give',\n",
       " 'itself',\n",
       " 'had',\n",
       " 'nevertheless',\n",
       " 'may',\n",
       " 'already',\n",
       " 'go',\n",
       " 'forty',\n",
       " 'should',\n",
       " 'whither',\n",
       " 'have',\n",
       " 'five',\n",
       " 'such',\n",
       " 'would',\n",
       " 'nine',\n",
       " 'mine',\n",
       " 'other',\n",
       " 'these',\n",
       " 'his',\n",
       " 'into',\n",
       " 'etc',\n",
       " 'seems',\n",
       " 'somehow',\n",
       " 'beyond',\n",
       " 'per',\n",
       " 'part',\n",
       " 'be',\n",
       " 'by',\n",
       " 'we',\n",
       " 'con',\n",
       " 'thereafter',\n",
       " 'were',\n",
       " 'do',\n",
       " 'neither',\n",
       " 'twenty',\n",
       " 'without',\n",
       " 'me',\n",
       " 'it',\n",
       " 'too',\n",
       " 'here',\n",
       " 'your',\n",
       " 'fifty',\n",
       " 'describe',\n",
       " 'her',\n",
       " 'latterly',\n",
       " 'themselves',\n",
       " 'done',\n",
       " 'in',\n",
       " 'another',\n",
       " 'eg',\n",
       " 'anywhere',\n",
       " 'thence',\n",
       " 'no',\n",
       " 'not',\n",
       " 'can',\n",
       " 'myself',\n",
       " 'everything',\n",
       " 'whereas',\n",
       " 'four',\n",
       " 'my',\n",
       " 'call',\n",
       " 'others',\n",
       " 'sometimes',\n",
       " 'anything',\n",
       " 'less',\n",
       " 'couldnt',\n",
       " 'sincere',\n",
       " 'that',\n",
       " 'you',\n",
       " 'seemed',\n",
       " 'throughout',\n",
       " 'show',\n",
       " 'twelve',\n",
       " 'yourselves',\n",
       " 'except',\n",
       " 'last',\n",
       " 'for',\n",
       " 'cry',\n",
       " 'due',\n",
       " 'down',\n",
       " 'between',\n",
       " 'move',\n",
       " 'hence',\n",
       " 'whenever']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_attendees = datadf.ideal_attendees.fillna('nan').apply(nlp.basic_clean)\n",
    "ideal_attendees = ideal_attendees.apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<726x796 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 3740 stored elements in Compressed Sparse Row format>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=0.3, max_features=None, min_df=2,\n",
       "                 ngram_range=(1, 3), preprocessor=None,\n",
       "                 stop_words=['nan', 'Nan', 'NaN', 'NAN', 'research',\n",
       "                             'conference', 'make', 'researcher', 'people',\n",
       "                             'like', 'event', 'don', 'am', 'however', 'whatever',\n",
       "                             'bottom', 'namely', 'most', 'whole', 'since',\n",
       "                             'among', 'than', 'anyone', 'hers', 'every', 'yet',\n",
       "                             'few', 'sixty', 'together', 'serious', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_att_matrix, ideal_att_vector = nlp.create_wordcount_matrix(ideal_attendees, ngram=(1,3), max_df=.3, stop_words=words_to_stop)\n",
    "ideal_att_matrix, ideal_att_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda22 = LatentDirichletAllocation(n_components= 4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=4, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda22.fit(ideal_att_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el32861112409022776606342436\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el32861112409022776606342436_data = {\"mdsDat\": {\"x\": [-0.11555031959043743, -0.07735323282257503, 0.05558599362029648, 0.13731755879271593], \"y\": [-0.11550098341408233, 0.11197043430428924, 0.06325226791596579, -0.05972171880617259], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [29.43374808587032, 25.30228868593829, 23.60472324340676, 21.65923998478462]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [15.0, 54.0, 27.0, 49.0, 15.0, 13.0, 13.0, 44.0, 25.0, 25.0, 22.0, 10.0, 12.0, 15.0, 15.0, 15.0, 18.0, 22.0, 17.0, 40.0, 14.0, 8.0, 17.0, 26.0, 7.0, 24.0, 11.0, 11.0, 9.0, 11.0, 11.893081462133313, 8.542427578464267, 7.753352588953829, 6.081660182128067, 6.072855447236269, 5.2390524985199844, 4.403878790584766, 4.4015312528853405, 4.384335124492306, 10.08168435725578, 3.565111168559953, 3.56508387573748, 3.5650448393451413, 3.5618919270901155, 3.561891927089518, 3.560022056880112, 3.556469746538624, 2.7262513406455544, 2.726232337035575, 2.7262280205456566, 2.7262280205456566, 2.7262280205456566, 2.72617308275118, 2.7242984557263843, 2.7232479491747528, 2.7222904237199312, 2.7222904237198695, 2.722095219152477, 2.72189526324217, 2.721895263238288, 6.091214359877721, 6.592531941881652, 13.928963770622982, 16.86698032493523, 13.024682620937604, 5.69453630182367, 16.293032533788374, 11.507431280188975, 32.71614716670881, 12.332095393735022, 7.708571902745997, 7.687933660348902, 6.079725178074105, 8.107222799429936, 20.463270486875825, 14.641284117744915, 8.23681990086444, 8.99119250234841, 8.441050939556149, 8.661301123650032, 7.5240472802092615, 6.6935360368053685, 6.751211078025775, 9.963602182385085, 8.298000374299761, 9.317531763247462, 7.4795680432156, 7.259635618014756, 6.89351682953007, 15.273040849424504, 10.250981257400385, 6.071266551818493, 5.228093727893581, 4.343642724747556, 3.5558364063527397, 3.551687563593476, 3.545819419323575, 3.5143055595249693, 3.510159058676281, 3.5005753794284766, 2.721625873774233, 2.721607274863522, 2.721583457847266, 2.721574784796887, 2.7215013305783606, 2.719103499229295, 2.7190099045820646, 2.7189055625221927, 2.7183065198937717, 2.717685807683103, 2.717662755064145, 2.7166370309419072, 2.7153439881284593, 2.711146328241992, 2.7096764046969737, 2.7096764046969635, 2.7096764046945463, 2.702962249587406, 2.6722139808989014, 6.678718121298701, 19.257760030376897, 18.656439145327315, 5.235524814678688, 28.4854549808534, 8.379164799417934, 14.699694904818234, 4.405353772775674, 11.114334588082547, 8.298814593134304, 9.303550294048641, 4.927538528046539, 9.374350535316804, 17.98064023623729, 3.570275988927644, 3.5660149045190006, 5.236575022260426, 6.190614104225726, 4.39493681186458, 6.608381210399039, 9.229112818342674, 8.312944932927328, 5.611009968308954, 5.594955711985482, 9.375124778525254, 5.28778841615345, 7.691274344473843, 5.255809243294027, 6.2104342573138345, 7.498478443619781, 6.083861498853975, 5.516982110871481, 5.479941674815955, 5.321691070714902, 7.63047186277732, 5.978713746806449, 4.33295672910167, 4.33002510306814, 4.329034604055833, 4.292931793865609, 3.5083519323400227, 3.5083519323400147, 3.5083519323399748, 3.5065036419533064, 3.5061776683748818, 3.5061776683748804, 3.5030105218190855, 7.5200025947481395, 2.6835272506211116, 2.683513730086616, 2.683483300905711, 2.683483300905711, 2.683483300905711, 2.6820601173207126, 2.6820601173207135, 2.6820601173207126, 2.681700658162075, 2.6809401295842052, 2.675244290558576, 2.675244290527597, 2.6752442904425426, 2.6691946942320968, 2.6513625969872203, 5.158304476589438, 5.158304475799465, 5.158304475079221, 7.573679826003023, 4.339277909685232, 10.111155299307262, 10.111155299276263, 10.111155299253916, 9.189038020869667, 7.374657762101562, 9.886200946944772, 10.748812311220663, 6.76880642599418, 7.103934314109518, 6.685566736878477, 4.346768939474994, 5.657941611794723, 11.392542307583563, 11.886453066680145, 8.810244083201164, 7.615169212170284, 4.346625496382935, 4.923708733794711, 7.324625023742233, 4.721594526962948, 5.7180591649611, 5.039232332364475, 5.518879314832326, 4.921151685040896, 4.6151073388968555, 14.923151170152774, 13.298055330503448, 12.479494749025088, 6.7477759830173625, 4.296782758206363, 4.293008561694616, 4.289564283244252, 4.283996594565965, 3.478334142492031, 3.476425081483507, 3.476416011230127, 3.4760783271273294, 3.4760783271270825, 3.4733461595088886, 3.4732879063017843, 2.6599173378564425, 2.6598846337275455, 2.65930989897348, 2.65930989897348, 2.65930989897348, 2.657885093280594, 2.654631537567222, 2.652039057880278, 2.649055183118553, 2.6423384845245117, 1.8414825664661703, 1.8414822132718356, 1.8414822132718356, 1.8414822132718356, 1.8414817106116426, 17.6265193586307, 5.121103794654267, 5.11782627803711, 28.454073163256147, 5.67609371166386, 5.569010742780144, 4.75246576059818, 4.249845217599259, 5.167425140876384, 6.511797956968616, 5.729161806672645, 6.725336242525037, 8.157768075319533, 5.695844615962777, 8.455165810906772, 6.134880965495945, 6.548585580730238, 5.645175598391803, 5.173939295069951, 5.112872613741956, 5.112872613710223, 5.112872613687349, 4.847683566404967, 5.22544452474254, 4.881309428094809, 4.303170428414601, 4.298878918392965], \"Term\": [\"hall\", \"leader\", \"big\", \"company\", \"speaker\", \"erika\", \"erika hall\", \"work\", \"doing\", \"want\", \"steve\", \"time\", \"thought\", \"ladner\", \"sam ladner\", \"sam\", \"portigal\", \"experience\", \"steve portigal\", \"industry\", \"le\", \"case\", \"hear\", \"different\", \"big company\", \"academic\", \"orgs\", \"study\", \"enterprise\", \"senior\", \"thought\", \"thought leader\", \"norman\", \"nielsen\", \"tomer sharon\", \"jeff\", \"quant\", \"cooper\", \"mind\", \"orgs\", \"qual\", \"nielsen norman\", \"potential\", \"sauro\", \"jeff sauro\", \"presenter\", \"microsoft\", \"qual quant\", \"present perspective\", \"young steve\", \"indi young steve\", \"young steve portigal\", \"thinker\", \"trying\", \"driven\", \"academic professional\", \"mix academic professional\", \"amy\", \"alan cooper\", \"alan\", \"industry leader\", \"tomer\", \"portigal\", \"steve\", \"steve portigal\", \"sharon\", \"academic\", \"design\", \"leader\", \"mix\", \"diverse\", \"large\", \"leader field\", \"perspective\", \"industry\", \"different\", \"jared spool\", \"jared\", \"practitioner\", \"spool\", \"professional\", \"client\", \"expert\", \"field\", \"team\", \"organization\", \"young\", \"ux\", \"new\", \"speaker\", \"time\", \"doe\", \"going\", \"come\", \"space\", \"year\", \"coming\", \"got\", \"doesn\", \"feel\", \"long time\", \"doing amazing\", \"doing interesting\", \"looking\", \"stuff\", \"want hear\", \"sale\", \"writer\", \"look\", \"ppl\", \"stage\", \"practicing\", \"conversation\", \"awesome\", \"mike monteiro\", \"monteiro\", \"mike\", \"theory\", \"advice\", \"cool\", \"want\", \"doing\", \"job\", \"work\", \"senior\", \"experience\", \"help\", \"hear\", \"know\", \"learn\", \"dont\", \"ux\", \"leader\", \"opportunity\", \"amazing\", \"learning\", \"thing\", \"sector\", \"really\", \"organization\", \"big\", \"small\", \"talk\", \"company\", \"great\", \"field\", \"topic\", \"team\", \"industry\", \"product\", \"agency\", \"folk\", \"le\", \"case\", \"case study\", \"wa\", \"environment\", \"genevieve\", \"particular\", \"kim goodwin\", \"goodwin\", \"kim\", \"medical\", \"bell\", \"genevieve bell\", \"conduct\", \"enterprise\", \"love hear\", \"recognition\", \"tricia\", \"tricia wang\", \"wang\", \"silicon valley\", \"silicon\", \"valley\", \"elizabeth\", \"highly\", \"laura klein\", \"klein\", \"laura\", \"international\", \"useful\", \"jan chipchase\", \"jan\", \"chipchase\", \"outside\", \"b2b\", \"ladner\", \"sam ladner\", \"sam\", \"le\", \"study\", \"working\", \"product\", \"data\", \"government\", \"love\", \"nonprofit\", \"method\", \"field\", \"industry\", \"just\", \"tech\", \"insight\", \"designer\", \"organization\", \"individual\", \"doing\", \"hear\", \"company\", \"folk\", \"young\", \"hall\", \"erika\", \"erika hall\", \"big company\", \"discipline\", \"kate\", \"hall steve\", \"reichelt\", \"tell\", \"erika hall steve\", \"leisa reichelt\", \"towsey\", \"kate towsey\", \"fan\", \"technology\", \"similar\", \"admit\", \"reichelt erika\", \"reichelt erika hall\", \"leisa reichelt erika\", \"rockstars\", \"slack\", \"talking\", \"hall steve portigal\", \"day day\", \"led\", \"ladner kate towsey\", \"sam ladner kate\", \"ladner kate\", \"nick\", \"big\", \"tech company\", \"did\", \"company\", \"real\", \"day\", \"story\", \"leisa\", \"problem\", \"user\", \"good\", \"google\", \"different\", \"background\", \"work\", \"just\", \"organization\", \"tech\", \"really\", \"ladner\", \"sam ladner\", \"sam\", \"way\", \"steve\", \"new\", \"folk\", \"indi young\"], \"Total\": [15.0, 54.0, 27.0, 49.0, 15.0, 13.0, 13.0, 44.0, 25.0, 25.0, 22.0, 10.0, 12.0, 15.0, 15.0, 15.0, 18.0, 22.0, 17.0, 40.0, 14.0, 8.0, 17.0, 26.0, 7.0, 24.0, 11.0, 11.0, 9.0, 11.0, 12.572830239349766, 9.217490664388496, 8.379735907463267, 6.702121729726404, 6.701926091980811, 5.863204567397905, 5.024403803308778, 5.024366878911415, 5.024212636015857, 11.733298738333014, 4.185546566315299, 4.185546148519998, 4.185545636348636, 4.185496264275938, 4.185496264275928, 4.185423426382261, 4.18533715665141, 3.346688025074325, 3.3466877713461947, 3.3466876436027233, 3.3466876436027233, 3.3466876436027233, 3.3466869056166955, 3.346684748288228, 3.3466157702153767, 3.3466806078231786, 3.346680607823179, 3.346679744923263, 3.3466197771651647, 3.346619777165103, 7.539556752120161, 8.35146451134079, 18.351479909512292, 22.515172334415855, 17.511042063573477, 7.51116910249796, 24.253110083706133, 16.719212689546712, 54.40467746131684, 18.39081779699661, 10.87900289106306, 10.879313532926176, 8.376447642205425, 11.69964286628832, 40.05794053579847, 26.582816656069973, 12.545346039996625, 14.200182768147915, 13.34478239332768, 14.217505432208839, 11.727834130869606, 10.014553339895114, 11.638966061700652, 31.618021116138387, 20.01522484430695, 32.419855186062605, 16.596279788795364, 20.06026009345176, 18.28881096884685, 15.903816137595909, 10.879264872477872, 6.692166156191618, 5.8546067108858315, 5.016096504252047, 4.179785265449074, 4.179691642326061, 4.179612075951873, 4.1789250572054675, 4.179084046294772, 4.179818389984468, 3.3424217613760145, 3.34242146864212, 3.342421368859375, 3.34242112332599, 3.3424200766794128, 3.3423647267862067, 3.342363479780684, 3.342383779505734, 3.3424274519701487, 3.3423326318807094, 3.3423322619653515, 3.3423102870893766, 3.342333816974202, 3.3421956701338056, 3.3422610219727993, 3.342261021972799, 3.3422610219727455, 3.342031734877426, 3.3416832096990388, 8.368636828886604, 25.91392836633885, 25.850324912304746, 6.693510064450854, 44.96775490179994, 11.655384455127294, 22.53455427490737, 5.843126950298806, 17.511766481702406, 12.560259197569332, 15.83919001353809, 7.4846825864273505, 20.06026009345176, 54.40467746131684, 5.018696262327815, 5.018703516202852, 9.143217612014856, 12.510445727840793, 7.533737955659633, 15.752170646791363, 32.419855186062605, 27.211901266705926, 13.345176700309315, 13.282531691302225, 49.51813730532358, 11.679138552654939, 31.618021116138387, 11.626562085342725, 20.01522484430695, 40.05794053579847, 20.745319452292225, 14.213285902397654, 16.582181786550457, 14.93879723995009, 8.261600925246416, 6.610189643944724, 4.958683950604695, 4.958672014145323, 4.958747444059593, 4.958525513095587, 4.132938699513293, 4.132938699513292, 4.132938699513292, 4.13298912491758, 4.13299404476968, 4.13299404476968, 4.1329682896751, 9.102079365542526, 3.307237773016113, 3.307237886384806, 3.307238041968935, 3.307238041968935, 3.307238041968935, 3.3072581568492088, 3.3072581568492097, 3.3072581568492088, 3.307262494445669, 3.3072779773199246, 3.3073398641724423, 3.3073398641729197, 3.3073398641742315, 3.307439205374625, 3.3072900900274727, 6.602928136345927, 6.602928136356944, 6.602928136366987, 9.9171924535766, 5.79744230507069, 15.649423012118508, 15.649423012119795, 15.649423012120717, 14.93879723995009, 11.533143084301615, 17.35808507681904, 20.745319452292225, 11.605359894437223, 12.418449125492385, 11.629249349692998, 6.616547277496477, 9.90825784116041, 31.618021116138387, 40.05794053579847, 24.866199482463287, 20.70406018069761, 6.633351526944754, 9.979162765387775, 32.419855186062605, 9.895099272203552, 25.850324912304746, 17.511766481702406, 49.51813730532358, 16.582181786550457, 16.596279788795364, 15.562379004902745, 13.925336098099251, 13.106889948553226, 7.377806937523241, 4.922358561752337, 4.92244030113328, 4.922492176402637, 4.922496449718512, 4.103909725811022, 4.103957655115654, 4.103957749100903, 4.103962231291852, 4.103962231291858, 4.103998991963233, 4.104021009980757, 3.28546034262176, 3.285460927855265, 3.2854755983913106, 3.2854755983913106, 3.2854755983913106, 3.2855109247083822, 3.2855082750111504, 3.2855784910071555, 3.2856449640571537, 3.285846590508127, 2.467011212907674, 2.4670112215639555, 2.4670112215639555, 2.4670112215639555, 2.46701123534541, 27.211901266705926, 7.392194029645567, 7.392232216663665, 49.51813730532358, 9.03869607564861, 9.067728247959918, 7.40715648736625, 6.5742365465827985, 9.043229485018909, 13.250914636645028, 12.381229147150645, 18.233911122721747, 26.582816656069973, 14.906848296558785, 44.96775490179994, 24.866199482463287, 32.419855186062605, 20.70406018069761, 15.752170646791363, 15.649423012118508, 15.649423012119795, 15.649423012120717, 13.25139314254936, 22.515172334415855, 18.28881096884685, 16.582181786550457, 12.4326258218629], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1674, 1.147, 1.1453, 1.1259, 1.1245, 1.1105, 1.0912, 1.0907, 1.0868, 1.0713, 1.0626, 1.0626, 1.0626, 1.0617, 1.0617, 1.0612, 1.0602, 1.018, 1.018, 1.018, 1.018, 1.018, 1.018, 1.0173, 1.0169, 1.0165, 1.0165, 1.0165, 1.0164, 1.0164, 1.0097, 0.9865, 0.9473, 0.9342, 0.927, 0.9461, 0.8252, 0.8495, 0.7144, 0.8234, 0.8785, 0.8758, 0.9026, 0.8562, 0.5513, 0.6266, 0.8023, 0.766, 0.765, 0.7274, 0.7792, 0.8201, 0.6784, 0.0682, 0.3425, -0.0238, 0.426, 0.2066, 0.2473, 1.3338, 1.3148, 1.2769, 1.2611, 1.2303, 1.2126, 1.2115, 1.2098, 1.2011, 1.1998, 1.1969, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1679, 1.1679, 1.1678, 1.1676, 1.1674, 1.1674, 1.167, 1.1665, 1.165, 1.1645, 1.1645, 1.1645, 1.162, 1.1507, 1.1487, 1.0774, 1.0481, 1.1286, 0.9177, 1.0443, 0.9471, 1.0918, 0.9196, 0.9599, 0.8422, 0.9563, 0.6135, 0.2671, 1.0337, 1.0326, 0.8169, 0.6707, 0.8353, 0.5056, 0.1179, 0.1884, 0.5079, 0.5097, -0.29, 0.5819, -0.0394, 0.5803, 0.204, -0.3014, 0.1476, 0.4279, 0.267, 0.3421, 1.3643, 1.3433, 1.3088, 1.3082, 1.3079, 1.2996, 1.2799, 1.2799, 1.2799, 1.2793, 1.2792, 1.2792, 1.2784, 1.2528, 1.2347, 1.2347, 1.2347, 1.2347, 1.2347, 1.2342, 1.2342, 1.2342, 1.2341, 1.2338, 1.2316, 1.2316, 1.2316, 1.2293, 1.2227, 1.1968, 1.1968, 1.1968, 1.1741, 1.154, 1.0069, 1.0069, 1.0069, 0.9578, 0.9965, 0.8808, 0.7862, 0.9046, 0.8852, 0.8902, 1.0236, 0.8834, 0.423, 0.2288, 0.4061, 0.4435, 1.021, 0.7373, -0.0438, 0.7038, -0.065, 0.1981, -0.7504, 0.2289, 0.1639, 1.4878, 1.4836, 1.4807, 1.4405, 1.3938, 1.3929, 1.3921, 1.3908, 1.3644, 1.3638, 1.3638, 1.3637, 1.3637, 1.3629, 1.3629, 1.3185, 1.3185, 1.3183, 1.3183, 1.3183, 1.3177, 1.3165, 1.3155, 1.3144, 1.3118, 1.2373, 1.2373, 1.2373, 1.2373, 1.2373, 1.0955, 1.1627, 1.162, 0.9757, 1.0645, 1.0422, 1.086, 1.0935, 0.9701, 0.8193, 0.7591, 0.5323, 0.3484, 0.5677, -0.1414, 0.1302, -0.0698, 0.2302, 0.4164, 0.4111, 0.4111, 0.4111, 0.5241, 0.0691, 0.2089, 0.1808, 0.4678], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.5726, -4.9035, -5.0004, -5.2432, -5.2447, -5.3924, -5.566, -5.5666, -5.5705, -4.7378, -5.7773, -5.7773, -5.7773, -5.7782, -5.7782, -5.7788, -5.7798, -6.0456, -6.0456, -6.0456, -6.0456, -6.0456, -6.0456, -6.0463, -6.0467, -6.047, -6.047, -6.0471, -6.0472, -6.0472, -5.2417, -5.1626, -4.4145, -4.2232, -4.4817, -5.309, -4.2578, -4.6055, -3.5607, -4.5363, -5.0062, -5.0089, -5.2436, -4.9558, -4.0299, -4.3647, -4.9399, -4.8523, -4.9154, -4.8897, -5.0304, -5.1474, -5.1388, -4.7496, -4.9325, -4.8166, -5.0363, -5.0662, -5.1179, -4.1712, -4.5699, -5.0937, -5.2432, -5.4286, -5.6287, -5.6299, -5.6315, -5.6404, -5.6416, -5.6443, -5.896, -5.8961, -5.8961, -5.8961, -5.8961, -5.897, -5.897, -5.897, -5.8973, -5.8975, -5.8975, -5.8979, -5.8984, -5.8999, -5.9004, -5.9004, -5.9004, -5.9029, -5.9144, -4.9983, -3.9394, -3.9711, -5.2418, -3.5479, -4.7715, -4.2094, -5.4145, -4.489, -4.7812, -4.6669, -5.3024, -4.6593, -4.008, -5.6246, -5.6258, -5.2416, -5.0742, -5.4168, -5.0089, -4.6749, -4.7795, -5.1725, -5.1754, -4.6592, -5.2319, -4.8572, -5.2379, -5.071, -4.8826, -5.0916, -5.1894, -5.1962, -5.2255, -4.7957, -5.0396, -5.3616, -5.3623, -5.3625, -5.3709, -5.5727, -5.5727, -5.5727, -5.5732, -5.5733, -5.5733, -5.5742, -4.8103, -5.8407, -5.8407, -5.8407, -5.8407, -5.8407, -5.8412, -5.8412, -5.8412, -5.8414, -5.8417, -5.8438, -5.8438, -5.8438, -5.846, -5.8528, -5.1872, -5.1872, -5.1872, -4.8031, -5.3601, -4.5142, -4.5142, -4.5142, -4.6098, -4.8298, -4.5367, -4.453, -4.9155, -4.8672, -4.9279, -5.3584, -5.0948, -4.3949, -4.3524, -4.6519, -4.7977, -5.3584, -5.2338, -4.8366, -5.2757, -5.0842, -5.2106, -5.1197, -5.2343, -5.2985, -4.0389, -4.1542, -4.2177, -4.8326, -5.2839, -5.2848, -5.2856, -5.2869, -5.4953, -5.4958, -5.4958, -5.4959, -5.4959, -5.4967, -5.4967, -5.7635, -5.7635, -5.7637, -5.7637, -5.7637, -5.7643, -5.7655, -5.7665, -5.7676, -5.7701, -6.1312, -6.1312, -6.1312, -6.1312, -6.1312, -3.8724, -5.1084, -5.1091, -3.3935, -5.0055, -5.0246, -5.1831, -5.2949, -5.0994, -4.8682, -4.9962, -4.8359, -4.6428, -5.0021, -4.607, -4.9278, -4.8626, -5.011, -5.0982, -5.11, -5.11, -5.11, -5.1633, -5.0883, -5.1564, -5.2825, -5.2835]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 4, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 3, 1, 2, 3, 4, 3, 2, 3, 4, 4, 3, 3, 3, 4, 1, 3, 2, 2, 1, 2, 3, 4, 3, 2, 1, 2, 1, 1, 2, 3, 4, 2, 3, 4, 4, 1, 2, 4, 1, 2, 3, 3, 4, 1, 3, 4, 4, 1, 2, 3, 2, 2, 2, 3, 4, 2, 2, 2, 4, 1, 3, 1, 3, 3, 4, 4, 4, 1, 2, 4, 1, 3, 4, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 3, 3, 2, 2, 3, 4, 3, 1, 2, 3, 4, 2, 2, 3, 4, 1, 2, 3, 4, 4, 4, 1, 2, 3, 2, 3, 3, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 2, 3, 3, 3, 4, 3, 4, 1, 2, 4, 1, 2, 4, 1, 1, 1, 2, 1, 2, 3, 4, 4, 4, 3, 3, 3, 1, 2, 3, 4, 4, 4, 1, 2, 3, 3, 3, 2, 3, 1, 2, 4, 1, 2, 1, 2, 3, 4, 2, 3, 4, 4, 3, 4, 4, 4, 2, 2, 2, 1, 2, 3, 3, 3, 1, 3, 4, 1, 2, 2, 1, 1, 2, 3, 1, 2, 1, 2, 3, 4, 4, 1, 1, 1, 3, 4, 1, 1, 2, 1, 2, 3, 4, 1, 2, 2, 3, 4, 3, 1, 2, 4, 1, 4, 1, 2, 2, 1, 3, 1, 1, 3, 4, 1, 2, 3, 4, 1, 2, 1, 1, 1, 3, 4, 1, 2, 3, 4, 3, 4, 4, 4, 4, 2, 3, 4, 3, 4, 4, 1, 1, 2, 2, 4, 1, 4, 3, 3, 4, 4, 1, 2, 3, 4, 2, 2, 1, 2, 4, 2, 1, 4, 1, 4, 2, 3, 4, 3, 4, 2, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 4, 4, 2, 1, 2, 3, 1, 1, 1, 2, 1, 4, 1, 1, 2, 4, 4, 3, 3, 1, 3, 1, 2, 4, 1, 2, 3, 3, 3, 3, 1, 2, 3, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 2, 2, 1, 3, 4, 1, 1], \"Freq\": [0.6597092061503986, 0.16492730153759966, 0.16492730153759966, 0.8964106084659587, 0.91311388748074, 0.8977511666254528, 0.42214024548594026, 0.42214024548594026, 0.1407134151619801, 0.896426902294015, 0.8964269022939985, 0.1992546474944189, 0.7970185899776756, 0.8964108395943299, 0.8976135140166387, 0.17248985800606545, 0.6899594320242618, 0.2683330453509339, 0.20124978401320043, 0.20124978401320043, 0.40249956802640086, 0.9678213800143303, 0.2939890131744705, 0.036748626646808814, 0.6614752796425587, 0.9487914307432295, 0.9683353229460653, 0.9076895404198139, 0.7572398028174, 0.15144796056348, 0.6989827466507171, 0.2995640342788788, 0.7974328238320929, 0.957026615703093, 0.12116772412105563, 0.18175158618158344, 0.12116772412105563, 0.5654493792315929, 0.9678274111109735, 0.8975764134522879, 0.11949377424866027, 0.8364564197406219, 0.7961202070631125, 0.0861670822013308, 0.25850124660399243, 0.6031695754093156, 0.0861670822013308, 0.2205624104857758, 0.1102812052428879, 0.6616872314573273, 0.9130067145149575, 0.7177371460501075, 0.17943428651252688, 0.11962285767501793, 0.3006264223292709, 0.2004176148861806, 0.5010440372154514, 0.2705542712107412, 0.6763856780268531, 0.5642742901954624, 0.1504731440521233, 0.3009462881042466, 0.8126185749816686, 0.7353615106189448, 0.1838403776547362, 0.0919201888273681, 0.8965706857784421, 0.9571475365628145, 0.735000432855527, 0.23210539984911377, 0.03868423330818563, 0.8975528754064547, 0.8975529022014873, 0.6680310009494524, 0.267212400379781, 0.8964279755984447, 0.9070946152711808, 0.1098650055486959, 0.8789200443895672, 0.8066675893443701, 0.9335501784961905, 0.9155490011056813, 0.7310016944888427, 0.17750517499492197, 0.6656444062309574, 0.1331288812461915, 0.6014279930787237, 0.08591828472553197, 0.2577548541765959, 0.7309943315957999, 0.9569793772821943, 0.31627532802474556, 0.25302026241979647, 0.3479028608272201, 0.09488259840742366, 0.12061139033116668, 0.3015284758279167, 0.3015284758279167, 0.24122278066233335, 0.8066553187321247, 0.9678213800143303, 0.8540283313485757, 0.24230227583586927, 0.24230227583586927, 0.48460455167173855, 0.967834340361993, 0.3290572143089611, 0.10968573810298704, 0.16452860715448056, 0.38390008336045467, 0.9571839516727016, 0.2415760591104448, 0.5636774712577045, 0.16105070607362987, 0.25686826014389824, 0.42811376690649705, 0.25686826014389824, 0.9638629155140371, 0.8125965175069522, 0.9130627419633204, 0.057104461794010006, 0.62814907973411, 0.28552230897005004, 0.6845649656465957, 0.17114124141164894, 0.9070903687482207, 0.40216765723033737, 0.24130059433820242, 0.3217341257842699, 0.8964087239317283, 0.10106012809887745, 0.5053006404943873, 0.4042405123955098, 0.4992767908806159, 0.17474687680821555, 0.29956607452836953, 0.7958027503822118, 0.13263379173036863, 0.30150671072925594, 0.6030134214585119, 0.9070461507274169, 0.7572398028185519, 0.15144796056371038, 0.7572398028198153, 0.15144796056396306, 0.6337946593326729, 0.2112648864442243, 0.14084325762948288, 0.6376866747632696, 0.2391325030362261, 0.0797108343454087, 0.8527759764348465, 0.95568117791451, 0.14939844571400396, 0.7469922285700197, 0.24129139654941875, 0.16086093103294583, 0.3619370948241281, 0.24129139654941875, 0.8126050810771825, 0.7310008793759417, 0.967834340361993, 0.9678343403619928, 0.9070733952980736, 0.31846476550213887, 0.6369295310042777, 0.6390011946291093, 0.31950059731455466, 0.8106975689928582, 0.8106975689928582, 0.7353405135156781, 0.18383512837891952, 0.09191756418945976, 0.9070733952977139, 0.9070733952982046, 0.334698966703206, 0.6024581400657707, 0.6065654928928468, 0.3308539052142801, 0.0735230900476178, 0.7162940970070061, 0.23876469900233535, 0.12626908309645618, 0.5682108739340528, 0.25253816619291236, 0.06313454154822809, 0.5468534395845106, 0.21874137583380426, 0.21874137583380426, 0.8106975718374444, 0.30421783363416904, 0.6084356672683381, 0.7310016777480813, 0.9131098101805748, 0.8975527967975395, 0.8975512686839892, 0.8975529681354897, 0.25797021886706706, 0.17198014591137806, 0.6019305106898232, 0.9071013957560359, 0.967822532095283, 0.10092591614298206, 0.6055554968578923, 0.30277774842894617, 0.955717508598592, 0.8975959628159956, 0.8975959628159812, 0.7961446478849578, 0.6524995316934579, 0.10874992194890964, 0.16312488292336447, 0.8964106084659585, 0.8975959628159813, 0.38274768173413765, 0.21871296099093582, 0.10935648049546791, 0.2733912012386698, 0.8106975644640616, 0.8952388873194839, 0.9556697878995776, 0.15113622831670795, 0.6045449132668318, 0.15113622831670795, 0.9546840244541523, 0.19925493549118897, 0.7970197419647559, 0.27760765581300706, 0.27760765581300706, 0.21591706563233884, 0.21591706563233884, 0.8522752401530289, 0.08522752401530288, 0.10083498980997929, 0.8066799184798343, 0.10083498980997929, 0.8066914225682418, 0.6837815556790562, 0.17094538891976405, 0.17094538891976405, 0.7628812536662642, 0.21796607247607552, 0.9556699048417253, 0.8975767317066581, 0.8975827323957182, 0.5994852343189918, 0.2997426171594959, 0.8964086897157004, 0.9556978093988128, 0.3317398950197853, 0.5528998250329755, 0.09640728862235055, 0.2892218658670517, 0.530240087422928, 0.09640728862235055, 0.6821378875868198, 0.3410689437934099, 0.9556696925059794, 0.8964086217547494, 0.7961143563671842, 0.3319062810489202, 0.6638125620978405, 0.06348331429508065, 0.4443832000655646, 0.19044994288524195, 0.31741657147540325, 0.9071013646615386, 0.812595812075951, 0.9131098101805748, 0.9131098101805748, 0.9130999922839326, 0.8975684476413832, 0.639001194629019, 0.3195005973145095, 0.6390011946290567, 0.31950059731452835, 0.8106975689928582, 0.9556811779145078, 0.398208700336635, 0.53094493378218, 0.6863780453403009, 0.25739176700261285, 0.7988104006345169, 0.13313506677241949, 0.9070958049607076, 0.9070958049607079, 0.9131140501321755, 0.9131007286809584, 0.37466720091342887, 0.44960064109611464, 0.07493344018268577, 0.14986688036537155, 0.9569869612835821, 0.9431698574872651, 0.6330224414482081, 0.2813433073103147, 0.07033582682757868, 0.8975768310466973, 0.7550464081509353, 0.2220724729855692, 0.7423887140927289, 0.2284272966439166, 0.13500457317266268, 0.13500457317266268, 0.6750228658633134, 0.6069464281188082, 0.34682653035360467, 0.8975532491955361, 0.07528685217855179, 0.4517211130713107, 0.22586055653565534, 0.22586055653565534, 0.9130812148336123, 0.39969573473342657, 0.29977180105006995, 0.14988590052503498, 0.09992393368335664, 0.19319882018741422, 0.14489911514056064, 0.38639764037482843, 0.2897982302811213, 0.27055566885544724, 0.676389172138618, 0.7309904098210419, 0.7310102318118449, 0.8976575442692586, 0.23979960948344062, 0.47959921896688124, 0.31973281264458747, 0.8964089216009852, 0.954439038112759, 0.9764045690625145, 0.9191797531557286, 0.8381763450583329, 0.11973947786547613, 0.8952650204810971, 0.17201989593478736, 0.4300497398369684, 0.34403979186957473, 0.7310008793759428, 0.9071013219882946, 0.9071013219882946, 0.896409499441036, 0.9070870465962301, 0.3018659548932655, 0.22639946616994913, 0.5282654210632147, 0.34894861618892964, 0.44864822081433814, 0.1495494069381127, 0.9070958049607079, 0.806665647547917, 0.9071013219882946, 0.11576785879739018, 0.7331964390501378, 0.11576785879739018, 0.8975681127668549, 0.22639129091772212, 0.1509275272784814, 0.22639129091772212, 0.37731881819620355, 0.08895262858319597, 0.6226684000823718, 0.08895262858319597, 0.17790525716639194, 0.17283012421723684, 0.5761004140574562, 0.23044016562298247, 0.8975629963246275, 0.9570083973405129, 0.42178127201289445, 0.30127233715206747, 0.24101786972165398, 0.8964087239317283, 0.8964087239317283], \"Term\": [\"academic\", \"academic\", \"academic\", \"academic professional\", \"admit\", \"advice\", \"agency\", \"agency\", \"agency\", \"alan\", \"alan cooper\", \"amazing\", \"amazing\", \"amy\", \"awesome\", \"b2b\", \"b2b\", \"background\", \"background\", \"background\", \"background\", \"bell\", \"big\", \"big\", \"big\", \"big company\", \"case\", \"case study\", \"chipchase\", \"chipchase\", \"client\", \"client\", \"come\", \"coming\", \"company\", \"company\", \"company\", \"company\", \"conduct\", \"conversation\", \"cool\", \"cool\", \"cooper\", \"data\", \"data\", \"data\", \"data\", \"day\", \"day\", \"day\", \"day day\", \"design\", \"design\", \"design\", \"designer\", \"designer\", \"designer\", \"did\", \"did\", \"different\", \"different\", \"different\", \"discipline\", \"diverse\", \"diverse\", \"diverse\", \"doe\", \"doesn\", \"doing\", \"doing\", \"doing\", \"doing amazing\", \"doing interesting\", \"dont\", \"dont\", \"driven\", \"elizabeth\", \"enterprise\", \"enterprise\", \"environment\", \"erika\", \"erika hall\", \"erika hall steve\", \"experience\", \"experience\", \"experience\", \"expert\", \"expert\", \"expert\", \"fan\", \"feel\", \"field\", \"field\", \"field\", \"field\", \"folk\", \"folk\", \"folk\", \"folk\", \"genevieve\", \"genevieve bell\", \"going\", \"good\", \"good\", \"good\", \"goodwin\", \"google\", \"google\", \"google\", \"google\", \"got\", \"government\", \"government\", \"government\", \"great\", \"great\", \"great\", \"hall\", \"hall steve\", \"hall steve portigal\", \"hear\", \"hear\", \"hear\", \"help\", \"help\", \"highly\", \"indi young\", \"indi young\", \"indi young\", \"indi young steve\", \"individual\", \"individual\", \"individual\", \"industry\", \"industry\", \"industry\", \"industry leader\", \"industry leader\", \"insight\", \"insight\", \"international\", \"jan\", \"jan\", \"jan chipchase\", \"jan chipchase\", \"jared\", \"jared\", \"jared\", \"jared spool\", \"jared spool\", \"jared spool\", \"jeff\", \"jeff sauro\", \"job\", \"job\", \"just\", \"just\", \"just\", \"just\", \"kate\", \"kate towsey\", \"kim\", \"kim goodwin\", \"klein\", \"know\", \"know\", \"ladner\", \"ladner\", \"ladner kate\", \"ladner kate towsey\", \"large\", \"large\", \"large\", \"laura\", \"laura klein\", \"le\", \"le\", \"leader\", \"leader\", \"leader\", \"leader field\", \"leader field\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"learning\", \"led\", \"leisa\", \"leisa\", \"leisa reichelt\", \"leisa reichelt erika\", \"long time\", \"look\", \"looking\", \"love\", \"love\", \"love\", \"love hear\", \"medical\", \"method\", \"method\", \"method\", \"microsoft\", \"mike\", \"mike monteiro\", \"mind\", \"mix\", \"mix\", \"mix\", \"mix academic professional\", \"monteiro\", \"new\", \"new\", \"new\", \"new\", \"nick\", \"nielsen\", \"nielsen norman\", \"nonprofit\", \"nonprofit\", \"nonprofit\", \"norman\", \"opportunity\", \"opportunity\", \"organization\", \"organization\", \"organization\", \"organization\", \"orgs\", \"orgs\", \"outside\", \"outside\", \"outside\", \"particular\", \"perspective\", \"perspective\", \"perspective\", \"portigal\", \"portigal\", \"potential\", \"ppl\", \"practicing\", \"practitioner\", \"practitioner\", \"present perspective\", \"presenter\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"product\", \"professional\", \"professional\", \"qual\", \"qual quant\", \"quant\", \"real\", \"real\", \"really\", \"really\", \"really\", \"really\", \"recognition\", \"reichelt\", \"reichelt erika\", \"reichelt erika hall\", \"rockstars\", \"sale\", \"sam\", \"sam\", \"sam ladner\", \"sam ladner\", \"sam ladner kate\", \"sauro\", \"sector\", \"sector\", \"senior\", \"senior\", \"sharon\", \"sharon\", \"silicon\", \"silicon valley\", \"similar\", \"slack\", \"small\", \"small\", \"small\", \"small\", \"space\", \"speaker\", \"spool\", \"spool\", \"spool\", \"stage\", \"steve\", \"steve\", \"steve portigal\", \"steve portigal\", \"story\", \"story\", \"story\", \"study\", \"study\", \"stuff\", \"talk\", \"talk\", \"talk\", \"talk\", \"talking\", \"team\", \"team\", \"team\", \"team\", \"tech\", \"tech\", \"tech\", \"tech\", \"tech company\", \"tech company\", \"technology\", \"tell\", \"theory\", \"thing\", \"thing\", \"thing\", \"thinker\", \"thought\", \"thought leader\", \"time\", \"tomer\", \"tomer\", \"tomer sharon\", \"topic\", \"topic\", \"topic\", \"towsey\", \"tricia\", \"tricia wang\", \"trying\", \"useful\", \"user\", \"user\", \"user\", \"ux\", \"ux\", \"ux\", \"valley\", \"wa\", \"wang\", \"want\", \"want\", \"want\", \"want hear\", \"way\", \"way\", \"way\", \"way\", \"work\", \"work\", \"work\", \"work\", \"working\", \"working\", \"working\", \"writer\", \"year\", \"young\", \"young\", \"young\", \"young steve\", \"young steve portigal\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el32861112409022776606342436\", ldavis_el32861112409022776606342436_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el32861112409022776606342436\", ldavis_el32861112409022776606342436_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el32861112409022776606342436\", ldavis_el32861112409022776606342436_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.115550 -0.115501       1        1  29.433748\n",
       "3     -0.077353  0.111970       2        1  25.302289\n",
       "2      0.055586  0.063252       3        1  23.604723\n",
       "1      0.137318 -0.059722       4        1  21.659240, topic_info=    Category       Freq        Term      Total  loglift  logprob\n",
       "279  Default  15.000000        hall  15.000000  30.0000  30.0000\n",
       "390  Default  54.000000      leader  54.000000  29.0000  29.0000\n",
       "64   Default  27.000000         big  27.000000  28.0000  28.0000\n",
       "104  Default  49.000000     company  49.000000  27.0000  27.0000\n",
       "645  Default  15.000000     speaker  15.000000  26.0000  26.0000\n",
       "..       ...        ...         ...        ...      ...      ...\n",
       "767   Topic4   4.847684         way  13.251393   0.5241  -5.1633\n",
       "667   Topic4   5.225445       steve  22.515172   0.0691  -5.0883\n",
       "472   Topic4   4.881309         new  18.288811   0.2089  -5.1564\n",
       "244   Topic4   4.303170        folk  16.582182   0.1808  -5.2825\n",
       "319   Topic4   4.298879  indi young  12.432626   0.4678  -5.2835\n",
       "\n",
       "[269 rows x 6 columns], token_table=      Topic      Freq                   Term\n",
       "term                                        \n",
       "4         1  0.659709               academic\n",
       "4         2  0.164927               academic\n",
       "4         3  0.164927               academic\n",
       "7         1  0.896411  academic professional\n",
       "15        4  0.913114                  admit\n",
       "...     ...       ...                    ...\n",
       "793       1  0.421781                  young\n",
       "793       3  0.301272                  young\n",
       "793       4  0.241018                  young\n",
       "794       1  0.896409            young steve\n",
       "795       1  0.896409   young steve portigal\n",
       "\n",
       "[375 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 3, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda22, ideal_att_matrix, ideal_att_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. industry\n",
    "2. field\n",
    "3. erika hall\n",
    "4. Indi young\n",
    "\n",
    "#### Topic groups (6) - some overlap between 2, 3, & 4; 5 is mostly overlapped with 2 & 4\n",
    "1. steve portigal\n",
    "2. expert\n",
    "3. diverse\n",
    "4. new\n",
    "5. startup\n",
    "6. doing research\n",
    "\n",
    "#### Topic groups (8)\n",
    "1. google\n",
    "2. jared spool\n",
    "3. different\n",
    "4. new\n",
    "5. consultant\n",
    "6. erika hall\n",
    "7. practioner\n",
    "8. steve portigal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W22 = lda22.transform(ideal_att_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_doc_column22 = datadf.ideal_attendees.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix22, count_vect22 = nlp.create_wordcount_matrix(datadf.ideal_attendees.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA22a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA22a.fit(word_count_matrix22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA22a.transform(word_count_matrix22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column22, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['nan', 'Nan', 'NaN', 'NAN', 'dont', 'research', 'conference', 'make', 'researcher', 'people', 'like', 'event']\n",
    "\n",
    "stopWords = nlp.set_stop_words(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = datadf.recommendations.fillna('nan').apply(nlp.basic_clean)\n",
    "recommendations = recommendations.apply(nlp.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<726x1211 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 6262 stored elements in Compressed Sparse Row format>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "                 ngram_range=(1, 3), preprocessor=None,\n",
       "                 stop_words=['nan', 'Nan', 'NaN', 'NAN', 'dont', 'research',\n",
       "                             'conference', 'make', 'researcher', 'people',\n",
       "                             'like', 'event', 'am', 'however', 'whatever',\n",
       "                             'bottom', 'namely', 'most', 'whole', 'since',\n",
       "                             'among', 'than', 'anyone', 'hers', 'every', 'yet',\n",
       "                             'few', 'sixty', 'together', 'serious', ...],\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations_matrix, recommendations_vector = nlp.create_wordcount_matrix(recommendations, ngram=(1,3), max_df=.7, stop_words=stopWords)\n",
    "recommendations_matrix, recommendations_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda23 = LatentDirichletAllocation(n_components= 3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=3, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda23.fit(recommendations_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el32861112389142832800129139\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el32861112389142832800129139_data = {\"mdsDat\": {\"x\": [-0.028140797981182325, -0.0710631828697508, 0.09920398085093315], \"y\": [-0.09241081131549952, 0.06911511339892658, 0.023295697916573047], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [40.12136235735325, 31.624672645125017, 28.25396499752173]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [35.0, 16.0, 28.0, 22.0, 10.0, 73.0, 19.0, 14.0, 25.0, 11.0, 13.0, 15.0, 24.0, 14.0, 13.0, 16.0, 8.0, 8.0, 7.0, 7.0, 30.0, 22.0, 8.0, 16.0, 17.0, 12.0, 35.0, 7.0, 12.0, 10.0, 8.867435988056418, 6.307737771951573, 20.420834308446654, 5.449741107576407, 5.443078170436479, 5.428043999964999, 25.54555917446127, 5.354428281861056, 4.587323698915688, 4.586680679920563, 4.585401716375642, 4.580949100129378, 4.575225575061542, 4.573198811458449, 4.56541770232656, 4.5621859584231785, 3.728635498802341, 3.728560889082813, 3.7284232557474364, 3.7247758825978523, 3.7175368405802205, 3.708252187889634, 3.684960840489762, 2.8682273689781623, 2.868106340705925, 2.8680852479936054, 2.8680107968048354, 2.8679721724947385, 2.8679002544056704, 2.867859951733253, 6.273934468664838, 10.213143793363015, 6.177891497128828, 10.574980360553285, 7.981881822331666, 16.239845045956233, 9.747249255163071, 28.018039019585725, 7.8609348687649305, 34.68624533557926, 13.005823214445854, 11.001050424841404, 27.135833846752252, 7.191821112246954, 19.994090047566605, 14.518893800158342, 11.285853957970815, 23.15447642151233, 12.988599682834337, 21.678335151078826, 14.614968490450089, 22.109774354378903, 13.149748397122583, 9.627554178593101, 9.942751550979857, 16.278455687978802, 18.275437048790188, 10.168378567905087, 16.184701891098307, 14.682646839248179, 14.882499240471388, 10.043578463499221, 23.648186153226444, 19.56082533340317, 15.199377846897534, 14.780709417164049, 13.16955663735816, 12.315980713078487, 11.980822073214354, 11.043631061261099, 11.00530648977432, 7.90081163444303, 7.882683385254691, 6.195471527702352, 6.182874842633257, 5.333240635452287, 6.858623921250396, 5.293447258288744, 4.515635256337925, 4.508931384117557, 4.496332080621983, 4.470732930981655, 4.471042774162375, 6.622847117649984, 3.67216666641655, 3.672004677407834, 3.6707793414672785, 3.663810191657395, 3.6594552168481744, 3.634126939749125, 4.2847916900773075, 4.26168480426218, 2.8246090616068393, 2.824570869279339, 2.8243070166803954, 2.820665916487815, 2.8204982434697423, 2.8186794347586632, 2.8180710609887165, 2.8170094067968425, 2.8170201293003947, 10.941854671909962, 7.544616337515274, 8.068724217592765, 10.401147459835368, 10.864226099188341, 6.56726880692335, 5.907513727795971, 9.297453750696205, 7.537229556881424, 11.348433230367633, 9.062413095599448, 13.878088296636642, 15.428965746254006, 18.241055629702988, 6.862622458424086, 6.732858234429735, 10.258102463338659, 31.290363173799182, 14.255498728098715, 17.620462824589044, 8.342022639791441, 14.844768423516044, 14.036052129592901, 15.448333494355268, 12.84716379141166, 8.692751178031921, 18.76170152353905, 16.350044360608358, 12.88903441972279, 11.057396054947269, 12.342270175822076, 12.42849500178132, 10.181729304156882, 11.705760370211497, 10.467859236975418, 9.480944530814858, 9.52932030369746, 10.04668800267188, 9.36987993810334, 9.18066671033439, 10.22424194731051, 6.873044807334658, 6.865628678802346, 15.194750330873704, 5.252877793445283, 5.246683493641431, 4.426226894231497, 4.418167769526478, 4.416047867473081, 4.393531660749206, 5.816485076979741, 3.5981249560967594, 3.5951502167060236, 3.593261673214708, 3.5915457740015024, 3.5795578445889604, 3.5448960006889925, 6.916177641659479, 2.767854370582673, 2.7678027990346226, 2.7677879343799128, 2.767742275929375, 2.7676098500309796, 2.7675743477781425, 2.7675743477781425, 2.7675575931473064, 2.7669428062623385, 2.7620813168894114, 2.758073498359038, 2.757515071366303, 28.441921019286823, 9.396647380204188, 10.559555994373246, 10.825525175252388, 5.6753087309380525, 12.87900235441092, 34.98543404669719, 7.784670667294797, 6.09015919685659, 14.377729605464879, 7.575881781107591, 8.71508429587526, 17.570051230491465, 9.100321673123826, 14.815398738106035, 8.129614144984657, 13.69637793856115, 8.199789804473511, 14.552593401155445, 8.916142439488867, 11.273681835158449, 17.1038641076099, 10.44140311457789, 10.162101088284036, 9.223663644387388, 6.844556269635605, 7.634376263517616, 9.358676333489113, 9.748786655196113, 7.863693412545743, 8.288479592094937, 7.8566565951154725, 7.755276350088274, 7.805293485741351], \"Term\": [\"attend\", \"real\", \"ux\", \"company\", \"europe\", \"talk\", \"year\", \"come\", \"sure\", \"insight\", \"track\", \"maybe\", \"consider\", \"survey\", \"audience\", \"having\", \"engage\", \"amazing\", \"live\", \"traditional\", \"think\", \"design\", \"kind\", \"product\", \"world\", \"smaller\", \"day\", \"participant\", \"hear\", \"multiple\", \"material\", \"apply\", \"company\", \"matter\", \"access\", \"local\", \"ux\", \"connect\", \"collective\", \"success\", \"sharing\", \"limited\", \"everyday\", \"cool\", \"prefer\", \"trend\", \"degree\", \"exchange\", \"corporate\", \"innovation\", \"worked\", \"series\", \"organized\", \"doubt\", \"supporting\", \"uxr collective\", \"method interesting\", \"written\", \"contribute\", \"mix speaker\", \"looking\", \"area\", \"advance\", \"look\", \"organization\", \"design\", \"type\", \"need\", \"say\", \"just\", \"thing\", \"field\", \"speaker\", \"uxr\", \"work\", \"interesting\", \"bring\", \"way\", \"doing\", \"focus\", \"ve\", \"good\", \"try\", \"professional\", \"let\", \"really\", \"lot\", \"opportunity\", \"new\", \"love\", \"different\", \"academic\", \"don\", \"talk\", \"topic\", \"experience\", \"time\", \"learn\", \"attendee\", \"sure\", \"know\", \"engage\", \"amazing\", \"author\", \"view\", \"interaction\", \"participant\", \"target\", \"continue\", \"mean\", \"style\", \"choose\", \"decision\", \"point\", \"consider having\", \"understanding\", \"different level\", \"lineup\", \"choice\", \"definitely\", \"hope\", \"volunteer\", \"forget\", \"organizing\", \"pilot\", \"sound\", \"great idea\", \"organisation\", \"thought leader\", \"senior practitioner\", \"push\", \"track\", \"remote\", \"multiple\", \"audience\", \"survey\", \"le\", \"doe\", \"smaller\", \"start\", \"having\", \"outside\", \"location\", \"consider\", \"think\", \"end\", \"price\", \"product\", \"don\", \"sure\", \"workshop\", \"understand\", \"great\", \"attendee\", \"day\", \"learn\", \"fun\", \"talk\", \"just\", \"good\", \"different\", \"way\", \"experience\", \"level\", \"topic\", \"want\", \"feel\", \"method\", \"time\", \"know\", \"idea\", \"europe\", \"live\", \"traditional\", \"real\", \"application\", \"north\", \"america\", \"welcome\", \"major\", \"near\", \"invite\", \"send\", \"north america\", \"excited\", \"able attend\", \"planning\", \"recording\", \"kind\", \"buck\", \"main\", \"attend want\", \"ideation\", \"likely attend\", \"north america europe\", \"america europe\", \"high quality\", \"day day\", \"global\", \"problem\", \"period\", \"attend\", \"insight\", \"come\", \"maybe\", \"relevant\", \"year\", \"talk\", \"hear\", \"money\", \"session\", \"practice\", \"option\", \"experience\", \"world\", \"time\", \"cost\", \"day\", \"industry\", \"topic\", \"use\", \"new\", \"don\", \"help\", \"love\", \"want\", \"hand\", \"accessible\", \"workshop\", \"speaker\", \"content\", \"focus\", \"lot\", \"level\", \"just\"], \"Total\": [35.0, 16.0, 28.0, 22.0, 10.0, 73.0, 19.0, 14.0, 25.0, 11.0, 13.0, 15.0, 24.0, 14.0, 13.0, 16.0, 8.0, 8.0, 7.0, 7.0, 30.0, 22.0, 8.0, 16.0, 17.0, 12.0, 35.0, 7.0, 12.0, 10.0, 9.450620806132038, 6.869779282499116, 22.313857800884325, 6.009334150547785, 6.009231403580355, 6.008653207576934, 28.303934563662068, 6.006712539679615, 5.148735524385628, 5.148760398700262, 5.148740421130407, 5.148668191054681, 5.14858268170986, 5.148379094519018, 5.148373477138789, 5.1483869612322, 4.288264728538489, 4.288262752359097, 4.288259046718435, 4.28812762772071, 4.288092248681043, 4.287944805160975, 4.287580553500433, 3.4277333371630845, 3.4277304571334133, 3.4277285141142784, 3.427727724067644, 3.4277273783040925, 3.427725412202003, 3.4277241729585066, 7.700007664840811, 12.852222412471551, 7.715293061949157, 13.696793442880075, 10.250024308707594, 22.275028175143582, 12.825137748147489, 40.983176180869386, 10.246519903572999, 58.84158318192897, 18.752987347481813, 15.398678366754782, 45.12754047502376, 9.392319706356123, 32.353199576662874, 22.12549631181325, 16.212339745087984, 40.91862966576969, 19.66320319197905, 38.294930010211445, 23.07464298771451, 40.06265327267269, 20.45838869560261, 13.681423526175585, 14.535847368958917, 28.965227987372316, 34.86225903374988, 15.368642748238873, 31.370453280036493, 27.133145375797373, 28.147053233400026, 15.367250772423612, 72.04241343463553, 73.30796090363941, 41.457731618264475, 44.779255649436834, 38.031643378136074, 32.245270333536645, 29.766241753073363, 25.585807204469532, 27.14694432554714, 8.47345750645114, 8.473371635600042, 6.778674311982276, 6.7781697711484235, 5.930589568441733, 7.628997447643033, 5.931774300601983, 5.083476555011111, 5.083625095277986, 5.08299950658347, 5.083057653626457, 5.084056748945727, 7.632631771493258, 4.236000318476095, 4.236000513335342, 4.235975295875443, 4.2360655012207555, 4.235902999403809, 4.235648196867642, 5.079755592784438, 5.087420582479092, 3.3885331970054495, 3.3885343980911204, 3.3885323184377136, 3.388588398233763, 3.388586943153118, 3.388417948295959, 3.388634504974501, 3.388380170498478, 3.388647275538914, 13.589233488657127, 9.296856928190108, 10.137541041262304, 13.506988930509934, 14.451389163354383, 8.489195501788727, 7.637721139841261, 12.752985377107692, 10.127563783587085, 16.164404084713073, 12.685121711249138, 21.202781903820686, 24.676788333791606, 30.596602360104125, 9.28287262951537, 9.282412817437395, 16.181752812572924, 72.04241343463553, 25.585807204469532, 37.25228172984318, 12.738399759568832, 30.521055056482755, 29.766241753073363, 35.40734282407279, 32.245270333536645, 15.34378620553466, 73.30796090363941, 58.84158318192897, 40.06265327267269, 28.147053233400026, 40.91862966576969, 44.779255649436834, 25.377676485235853, 41.457731618264475, 29.622665067928907, 21.971293011218457, 26.292038457791723, 38.031643378136074, 27.14694432554714, 23.71946248488224, 10.8114324213711, 7.49069813537677, 7.491137447867386, 16.654838584241364, 5.828719824090557, 5.8290376036080165, 4.998270060620878, 4.9984455521549584, 4.99844429512205, 4.999182495099009, 6.666093731649319, 4.167778548726507, 4.167885269986117, 4.1678802918428115, 4.1679145741077335, 4.168162720876895, 4.169705872598168, 8.337144864518248, 3.3373663837552097, 3.3373678117442243, 3.3373682456646354, 3.3373696405434585, 3.3373742391695105, 3.3373740984492026, 3.3373740984492026, 3.3373751705942887, 3.3373993094318517, 3.337489607242505, 3.3377199339494887, 3.3375787666223724, 35.875858305329395, 11.676027145294405, 14.241557048569646, 15.072739438397353, 7.533123990138977, 19.31451056355557, 73.30796090363941, 12.621084111472891, 9.201518306600784, 28.638814845975773, 12.598567833967333, 15.971745740202916, 44.779255649436834, 17.62939000373029, 38.031643378136074, 15.174536708229365, 35.40734282407279, 16.044689761737338, 41.457731618264475, 19.403595160222146, 31.370453280036493, 72.04241343463553, 27.910068983531957, 27.133145375797373, 29.622665067928907, 12.588230042307275, 17.742534037894067, 37.25228172984318, 45.12754047502376, 23.734112434167624, 38.294930010211445, 34.86225903374988, 25.377676485235853, 58.84158318192897], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8496, 0.8279, 0.8246, 0.8155, 0.8143, 0.8116, 0.8107, 0.7983, 0.7978, 0.7977, 0.7974, 0.7964, 0.7952, 0.7948, 0.7931, 0.7924, 0.7734, 0.7734, 0.7734, 0.7724, 0.7705, 0.768, 0.7618, 0.7351, 0.735, 0.735, 0.735, 0.735, 0.7349, 0.7349, 0.7084, 0.6834, 0.691, 0.6546, 0.6632, 0.5973, 0.6388, 0.5329, 0.6482, 0.3848, 0.5473, 0.577, 0.4046, 0.6463, 0.432, 0.492, 0.551, 0.3439, 0.4986, 0.3443, 0.4566, 0.3188, 0.4713, 0.5619, 0.5335, 0.337, 0.2674, 0.5002, 0.2515, 0.2992, 0.276, 0.488, -0.2007, -0.4079, -0.0902, -0.1952, -0.1472, -0.0492, 0.0032, 0.0731, 0.0104, 1.0813, 1.079, 1.0613, 1.0593, 1.0451, 1.0448, 1.0374, 1.0328, 1.0313, 1.0286, 1.0229, 1.0227, 1.0093, 1.0084, 1.0084, 1.008, 1.0061, 1.005, 0.9981, 0.981, 0.9741, 0.9692, 0.9692, 0.9691, 0.9678, 0.9677, 0.9671, 0.9669, 0.9666, 0.9665, 0.9346, 0.9424, 0.923, 0.8899, 0.8659, 0.8945, 0.8944, 0.8352, 0.8558, 0.7975, 0.8149, 0.7274, 0.6816, 0.634, 0.8492, 0.8301, 0.6954, 0.3173, 0.5663, 0.4026, 0.7279, 0.4305, 0.3995, 0.3218, 0.231, 0.583, -0.2116, -0.1294, 0.0172, 0.2169, -0.0473, -0.1305, 0.238, -0.1134, 0.111, 0.3108, 0.1363, -0.1799, 0.0875, 0.202, 1.2081, 1.1779, 1.1767, 1.1722, 1.1599, 1.1587, 1.1424, 1.1405, 1.1401, 1.1348, 1.1276, 1.117, 1.1161, 1.1156, 1.1151, 1.1117, 1.1016, 1.0771, 1.0768, 1.0768, 1.0768, 1.0768, 1.0767, 1.0767, 1.0767, 1.0767, 1.0765, 1.0747, 1.0732, 1.073, 1.0317, 1.0468, 0.9648, 0.933, 0.9808, 0.8587, 0.5242, 0.7807, 0.8512, 0.5749, 0.7553, 0.6582, 0.3284, 0.6027, 0.3212, 0.6398, 0.3141, 0.5927, 0.217, 0.4863, 0.2405, -0.174, 0.2807, 0.2818, 0.0972, 0.6546, 0.4206, -0.1175, -0.2684, 0.1593, -0.2665, -0.2261, 0.0784, -0.7561], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.7203, -6.0609, -4.8861, -6.2071, -6.2084, -6.2111, -4.6622, -6.2248, -6.3794, -6.3795, -6.3798, -6.3808, -6.382, -6.3825, -6.3842, -6.3849, -6.5867, -6.5867, -6.5867, -6.5877, -6.5896, -6.5921, -6.5984, -6.849, -6.8491, -6.8491, -6.8491, -6.8491, -6.8491, -6.8491, -6.0663, -5.579, -6.0817, -5.5442, -5.8255, -5.1152, -5.6257, -4.5699, -5.8408, -4.3564, -5.3373, -5.5047, -4.6018, -5.9298, -4.9073, -5.2273, -5.4792, -4.7605, -5.3386, -4.8264, -5.2207, -4.8067, -5.3263, -5.6381, -5.6059, -5.1129, -4.9971, -5.5834, -5.1186, -5.216, -5.2025, -5.5958, -4.7394, -4.9292, -5.1814, -5.2094, -5.3248, -5.3918, -5.4194, -5.5008, -5.5043, -5.5978, -5.6001, -5.8409, -5.8429, -5.9908, -5.7392, -5.9983, -6.1572, -6.1587, -6.1615, -6.1672, -6.1671, -5.7742, -6.3639, -6.364, -6.3643, -6.3662, -6.3674, -6.3744, -6.2097, -6.2151, -6.6264, -6.6264, -6.6265, -6.6278, -6.6278, -6.6285, -6.6287, -6.6291, -6.6291, -5.2721, -5.6439, -5.5767, -5.3228, -5.2793, -5.7826, -5.8885, -5.435, -5.6449, -5.2357, -5.4606, -5.0344, -4.9285, -4.7611, -5.7386, -5.7577, -5.3367, -4.2214, -5.0076, -4.7957, -5.5434, -4.9671, -5.0231, -4.9272, -5.1116, -5.5022, -4.7329, -4.8705, -5.1084, -5.2616, -5.1517, -5.1447, -5.3441, -5.2046, -5.3164, -5.4154, -5.4104, -5.3575, -5.4272, -5.4476, -5.2273, -5.6244, -5.6255, -4.8311, -5.8933, -5.8944, -6.0645, -6.0663, -6.0668, -6.0719, -5.7913, -6.2716, -6.2724, -6.273, -6.2734, -6.2768, -6.2865, -5.6182, -6.534, -6.534, -6.534, -6.534, -6.534, -6.5341, -6.5341, -6.5341, -6.5343, -6.536, -6.5375, -6.5377, -4.2042, -5.3117, -5.195, -5.1701, -5.8159, -4.9964, -3.9971, -5.4999, -5.7454, -4.8863, -5.5271, -5.387, -4.6858, -5.3437, -4.8564, -5.4565, -4.9349, -5.4479, -4.8743, -5.3642, -5.1296, -4.7127, -5.2062, -5.2334, -5.3303, -5.6286, -5.5194, -5.3157, -5.2749, -5.4898, -5.4372, -5.4907, -5.5037, -5.4972]}, \"token.table\": {\"Topic\": [3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 2, 3, 3, 3, 1, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 2, 3, 2, 1, 2, 3, 3, 2, 2, 1, 1, 3, 1, 3, 1, 1, 2, 3, 2, 1, 2, 3, 2, 1, 1, 1, 1, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 2, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 1, 3, 1, 2, 3, 3, 2, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 1, 1, 2, 3, 2, 1, 2, 3, 1, 1, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 3, 3, 3, 3, 1, 2, 3, 1, 2, 3, 2, 1, 3, 1, 2, 1, 2, 3, 2, 3, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 3, 1, 2, 1, 2, 3, 2, 1, 3, 1, 2, 3, 3, 1, 3, 2, 3, 1, 3, 3, 2, 1, 1, 2, 3, 1, 1, 2, 2, 1, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 3, 1, 3, 1, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3], \"Freq\": [0.9597125682107627, 0.6507344838769018, 0.19522034516307055, 0.13014689677538036, 0.8320531635744556, 0.39453214434023753, 0.16908520471724467, 0.4508938792459858, 0.7776762271793972, 0.12961270452989954, 0.9441342058441982, 0.8002768860998931, 0.8989103143678222, 0.8578212971113497, 0.8733905054686555, 0.7780755482644146, 0.15561510965288292, 0.07780755482644146, 0.027873897580074036, 0.1672433854804442, 0.780469132242073, 0.8989118907981194, 0.4031412530861744, 0.47033146193387015, 0.13438041769539147, 0.7403574587532047, 0.2221072376259614, 0.8851288207480542, 0.6784955270464759, 0.12336282309935925, 0.18504423464903888, 0.8989123922991025, 0.9443086870882049, 0.7869279226345661, 0.9711122228591503, 0.210651124014653, 0.7723874547203943, 0.8963039998940647, 0.044815199994703236, 0.8324020780036677, 0.324191296362706, 0.6078586806800738, 0.04052391204533825, 0.9442869922727021, 0.4634679316747654, 0.2106672416703479, 0.33706758667255665, 0.9835788452827972, 0.8752159637176923, 0.971179454388473, 0.9327794698086106, 0.3953992214303397, 0.06589987023838995, 0.5271989619071196, 0.16945637603510624, 0.42364094008776565, 0.3953982107485813, 0.8989035239270516, 0.7867732791986387, 0.9443654935643832, 0.9327782339042454, 0.7182931430746379, 0.26935992865298924, 0.5329154663409174, 0.39080467531667273, 0.07105539551212231, 0.9442925703308016, 0.13092910590616083, 0.785574635436965, 0.6611333806133335, 0.25428206946666676, 0.05085641389333335, 0.3331370904415262, 0.430302075153638, 0.23597210572941438, 0.8752139402077608, 0.7540769198688713, 0.2154505485339632, 0.9441246378953715, 0.9249468165044319, 0.9711410516455927, 0.93277866376063, 0.9597204621804087, 0.334976537292858, 0.2679812298342864, 0.4019718447514296, 0.22756967454974178, 0.4096254141895352, 0.3185975443696385, 0.7143470197902583, 0.19482191448825226, 0.06494063816275075, 0.5744885809723022, 0.20890493853538258, 0.20890493853538258, 0.8853388252625625, 0.39103777383418825, 0.5865566607512824, 0.8988792035456419, 0.5491398647579468, 0.32449173826605954, 0.1248045147177152, 0.29487840388690545, 0.4914640064781758, 0.1965856025912703, 0.8853247829635047, 0.07943928547850972, 0.39719642739254857, 0.556074998349568, 0.3093216411688555, 0.6805076105714822, 0.3961624814349245, 0.6338599702958793, 0.3582936325202347, 0.2508055427641643, 0.3582936325202347, 0.8989100255892981, 0.787439459820039, 0.3372757711140737, 0.37943524250333294, 0.2529568283355553, 0.898911515091112, 0.4362814179613051, 0.06232591685161501, 0.4986073348129201, 0.9328080568642356, 0.17129113996673317, 0.7708101298502993, 0.843086499630045, 0.6779508937836208, 0.04519672625224139, 0.2711803575134483, 0.9000773528750675, 0.5948174421443672, 0.2719165449802821, 0.13595827249014106, 0.11994513904344685, 0.839615973304128, 0.4052021423880199, 0.3315290255901981, 0.2578559087923763, 0.23559358476060394, 0.8245775466621138, 0.3721476010551358, 0.40315990114306377, 0.21708610061549585, 0.6879543893226926, 0.27518175572907705, 0.06879543893226926, 0.2758329748616836, 0.3940471069452623, 0.31523768555620985, 0.8989102764652895, 0.9711249228853049, 0.944272461992685, 0.9344923361603212, 0.832133229738568, 0.2358181121081575, 0.6602907139028411, 0.14149086726489452, 0.8031076796093517, 0.14601957811079122, 0.07300978905539561, 0.7792200035587943, 0.1298700005931324, 0.5163176598101213, 0.25815882990506067, 0.22947451547116504, 0.5528293823752525, 0.07371058431670033, 0.36855292158350167, 0.8989120076735252, 0.8002489902515418, 0.9523183909950494, 0.8320389372164005, 0.1990348212586751, 0.13268988083911673, 0.729794344615142, 0.9835501057393743, 0.3803432744879647, 0.3803432744879647, 0.26624029214157524, 0.8752153734194311, 0.8752162801392117, 0.32603315018652146, 0.6520663003730429, 0.7891460036943887, 0.19728650092359717, 0.8001308221737121, 0.6832071744861535, 0.17080179362153838, 0.14640153738989004, 0.510034071142417, 0.12750851778560424, 0.3506484239104117, 0.8577745315805023, 0.9597193158854211, 0.8989103143678222, 0.6506755452524213, 0.1952026635757264, 0.13013510905048428, 0.187831690336055, 0.25044225378140667, 0.5634950710081651, 0.8853689378869879, 0.7804859538922112, 0.1951214884730528, 0.9329270785908269, 0.8853385114490809, 0.07883251124923793, 0.7094926012431414, 0.15766502249847586, 0.9175517553964629, 0.8988551910749355, 0.885339054810359, 0.9596554328278439, 0.13101640822433633, 0.9171148575703543, 0.23812230402186035, 0.15874820268124024, 0.6349928107249609, 0.9711805140404756, 0.7541142736994213, 0.21546122105697751, 0.8988171744086784, 0.37078801471606404, 0.6179800245267734, 0.7309180934913527, 0.21927542804740582, 0.07309180934913528, 0.8853090204033981, 0.060042611337355715, 0.9006391700603358, 0.5523864685952191, 0.27619323429760956, 0.172620771436006, 0.9593002773376857, 0.1327470517290067, 0.7964823103740402, 0.8605058743823675, 0.10756323429779593, 0.780752887349623, 0.19518822183740575, 0.9597438907166089, 0.8853788090604539, 0.9328478284481637, 0.27934116837674, 0.20950587628255501, 0.488847044659295, 0.9711112992762313, 0.23523903707951901, 0.7057171112385571, 0.885324402799612, 0.598304266436665, 0.1772753382034563, 0.2215941727543204, 0.7899234377536033, 0.19748085943840082, 0.7869369247073945, 0.9711075312928108, 0.8752146755754181, 0.4299258535051586, 0.5471783590065655, 0.20759249966137203, 0.761172498758364, 0.2728216656617861, 0.2591805823786968, 0.4774379149081257, 0.8429181129653867, 0.6932228854591354, 0.05332483734301041, 0.21329934937204165, 0.32683367526583, 0.588300615478494, 0.09805010257974901, 0.8853123568198379, 0.34182062212629866, 0.2629389400971528, 0.39440841014572925, 0.36181429650125024, 0.2894514372010002, 0.36181429650125024, 0.14717533565593607, 0.8094643461076485, 0.9344375335140586, 0.9711779704304345, 0.6354361623207531, 0.14663911438171223, 0.1955188191756163, 0.7797187208725643, 0.07797187208725644, 0.15594374417451287, 0.23550838854358136, 0.6280223694495504, 0.0785027961811938, 0.9442869488347819, 0.30922104643268117, 0.20614736428845412, 0.4638315696490218, 0.9186002017323779, 0.07066155397941369, 0.7452897919629851, 0.2129399405608529, 0.8752151716937235, 0.6500642288587676, 0.3033633068007582, 0.04333761525725117, 0.8851947063260739, 0.19656326497635498, 0.7862530599054199, 0.33757934936200384, 0.33757934936200384, 0.30382141442580346, 0.5620911596470337, 0.2932649528593219, 0.1221937303580508, 0.800248789001112, 0.6181768808555946, 0.15454422021389866, 0.24727075234223786, 0.9328157530263823, 0.2684399326871003, 0.4831918788367805, 0.24159593941839025, 0.05672346007368406, 0.39706422051578844, 0.5105111406631565, 0.8752154617046249, 0.31064727114138546, 0.051774545190230906, 0.6730690874730018], \"Term\": [\"able attend\", \"academic\", \"academic\", \"academic\", \"access\", \"accessible\", \"accessible\", \"accessible\", \"advance\", \"advance\", \"amazing\", \"america\", \"america europe\", \"application\", \"apply\", \"area\", \"area\", \"area\", \"attend\", \"attend\", \"attend\", \"attend want\", \"attendee\", \"attendee\", \"attendee\", \"audience\", \"audience\", \"author\", \"bring\", \"bring\", \"bring\", \"buck\", \"choice\", \"choose\", \"collective\", \"come\", \"come\", \"company\", \"company\", \"connect\", \"consider\", \"consider\", \"consider\", \"consider having\", \"content\", \"content\", \"content\", \"continue\", \"contribute\", \"cool\", \"corporate\", \"cost\", \"cost\", \"cost\", \"day\", \"day\", \"day\", \"day day\", \"decision\", \"definitely\", \"degree\", \"design\", \"design\", \"different\", \"different\", \"different\", \"different level\", \"doe\", \"doe\", \"doing\", \"doing\", \"doing\", \"don\", \"don\", \"don\", \"doubt\", \"end\", \"end\", \"engage\", \"europe\", \"everyday\", \"exchange\", \"excited\", \"experience\", \"experience\", \"experience\", \"feel\", \"feel\", \"feel\", \"field\", \"field\", \"field\", \"focus\", \"focus\", \"focus\", \"forget\", \"fun\", \"fun\", \"global\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"great idea\", \"hand\", \"hand\", \"hand\", \"having\", \"having\", \"hear\", \"hear\", \"help\", \"help\", \"help\", \"high quality\", \"hope\", \"idea\", \"idea\", \"idea\", \"ideation\", \"industry\", \"industry\", \"industry\", \"innovation\", \"insight\", \"insight\", \"interaction\", \"interesting\", \"interesting\", \"interesting\", \"invite\", \"just\", \"just\", \"just\", \"kind\", \"kind\", \"know\", \"know\", \"know\", \"le\", \"le\", \"learn\", \"learn\", \"learn\", \"let\", \"let\", \"let\", \"level\", \"level\", \"level\", \"likely attend\", \"limited\", \"lineup\", \"live\", \"local\", \"location\", \"location\", \"location\", \"look\", \"look\", \"look\", \"looking\", \"looking\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"main\", \"major\", \"material\", \"matter\", \"maybe\", \"maybe\", \"maybe\", \"mean\", \"method\", \"method\", \"method\", \"method interesting\", \"mix speaker\", \"money\", \"money\", \"multiple\", \"multiple\", \"near\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"north\", \"north america\", \"north america europe\", \"opportunity\", \"opportunity\", \"opportunity\", \"option\", \"option\", \"option\", \"organisation\", \"organization\", \"organization\", \"organized\", \"organizing\", \"outside\", \"outside\", \"outside\", \"participant\", \"period\", \"pilot\", \"planning\", \"point\", \"point\", \"practice\", \"practice\", \"practice\", \"prefer\", \"price\", \"price\", \"problem\", \"product\", \"product\", \"professional\", \"professional\", \"professional\", \"push\", \"real\", \"real\", \"really\", \"really\", \"really\", \"recording\", \"relevant\", \"relevant\", \"remote\", \"remote\", \"say\", \"say\", \"send\", \"senior practitioner\", \"series\", \"session\", \"session\", \"session\", \"sharing\", \"smaller\", \"smaller\", \"sound\", \"speaker\", \"speaker\", \"speaker\", \"start\", \"start\", \"style\", \"success\", \"supporting\", \"sure\", \"sure\", \"survey\", \"survey\", \"talk\", \"talk\", \"talk\", \"target\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"thought leader\", \"time\", \"time\", \"time\", \"topic\", \"topic\", \"topic\", \"track\", \"track\", \"traditional\", \"trend\", \"try\", \"try\", \"try\", \"type\", \"type\", \"type\", \"understand\", \"understand\", \"understand\", \"understanding\", \"use\", \"use\", \"use\", \"ux\", \"ux\", \"uxr\", \"uxr\", \"uxr collective\", \"ve\", \"ve\", \"ve\", \"view\", \"volunteer\", \"volunteer\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"welcome\", \"work\", \"work\", \"work\", \"worked\", \"workshop\", \"workshop\", \"workshop\", \"world\", \"world\", \"world\", \"written\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el32861112389142832800129139\", ldavis_el32861112389142832800129139_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el32861112389142832800129139\", ldavis_el32861112389142832800129139_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el32861112389142832800129139\", ldavis_el32861112389142832800129139_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1     -0.028141 -0.092411       1        1  40.121362\n",
       "2     -0.071063  0.069115       2        1  31.624673\n",
       "0      0.099204  0.023296       3        1  28.253965, topic_info=     Category       Freq     Term      Total  loglift  logprob\n",
       "71    Default  35.000000   attend  35.000000  30.0000  30.0000\n",
       "857   Default  16.000000     real  16.000000  29.0000  29.0000\n",
       "1133  Default  28.000000       ux  28.000000  28.0000  28.0000\n",
       "188   Default  22.000000  company  22.000000  27.0000  27.0000\n",
       "341   Default  10.000000   europe  10.000000  26.0000  26.0000\n",
       "...       ...        ...      ...        ...      ...      ...\n",
       "203    Topic3   7.863693  content  23.734112   0.1593  -5.4898\n",
       "385    Topic3   8.288480    focus  38.294930  -0.2665  -5.4372\n",
       "627    Topic3   7.856657      lot  34.862259  -0.2261  -5.4907\n",
       "588    Topic3   7.755276    level  25.377676   0.0784  -5.5037\n",
       "548    Topic3   7.805293     just  58.841583  -0.7561  -5.4972\n",
       "\n",
       "[235 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "5         3  0.959713  able attend\n",
       "8         1  0.650734     academic\n",
       "8         2  0.195220     academic\n",
       "8         3  0.130147     academic\n",
       "9         1  0.832053       access\n",
       "...     ...       ...          ...\n",
       "1200      3  0.510511        world\n",
       "1206      1  0.875215      written\n",
       "1207      1  0.310647         year\n",
       "1207      2  0.051775         year\n",
       "1207      3  0.673069         year\n",
       "\n",
       "[331 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda23, recommendations_matrix, recommendations_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic groups (4)\n",
    "1. pratical\n",
    "2. affordable\n",
    "3. expensive\n",
    "4. levels\n",
    "\n",
    "#### Topic groups (6) - some overlap between 2, 3, & 4; 5 is mostly overlapped with 2 & 4\n",
    "1. ux\n",
    "2. interesting\n",
    "3. content\n",
    "4. food\n",
    "5. learning\n",
    "6. affordalbe\n",
    "\n",
    "#### Topic groups (8) - significant overlap amoung 6 of the 8 groups\n",
    "1. events\n",
    "2. ux\n",
    "3. world\n",
    "4. new\n",
    "5. food\n",
    "6. diverse\n",
    "7. make accessible\n",
    "8. north america"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 topics, 4. misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W23 = lda23.transform(recommendations_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc_column23 = datadf.recommendations.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_matrix23, count_vect23 = nlp.create_wordcount_matrix(datadf.recommendations.dropna(), max_df=0.8, min_df=2, ngram=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA23a = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA23a.fit(word_count_matrix23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda23.fit(recommendations_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_H = LDA23a.transform(word_count_matrix23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.find_top_documents_per_topic(lda_H, top_doc_column23, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
